{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1a7ec150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fc2c39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f60076d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "41ed15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"deepar-gas-prices\"  # prefix used for all data stored within the bucket\n",
    "role = sagemaker.get_execution_role()  # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6b5fc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6e4a665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c157056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('DeepAR.csv', index_col=0, parse_dates=True)\n",
    "num_timeseries = data.shape[1]\n",
    "data_kw = data.resample(\"7D\").sum() / 7\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(data_kw.iloc[:,i])\n",
    "timeseries[0].index.freq='7D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "90f3f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 1 day frequency for the time series\n",
    "freq = \"7D\"\n",
    "\n",
    "# we predict for \n",
    "prediction_length = 153\n",
    "\n",
    "# we use 100 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a21d2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2015-07-30\")\n",
    "end_training = pd.Timestamp(\"2021-04-23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bb660e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "2021-04-22 00:00:00\n",
      "Date\n",
      "2015-07-30      0.000000\n",
      "2015-08-06    335.321429\n",
      "2015-08-13    171.527143\n",
      "2015-08-20    147.735714\n",
      "2015-08-27     62.555714\n",
      "                 ...    \n",
      "2024-02-29     72.615714\n",
      "2024-03-07     66.354286\n",
      "2024-03-14     42.375714\n",
      "2024-03-21     29.847143\n",
      "2024-03-28     16.537143\n",
      "Freq: 7D, Name: Value, Length: 453, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": start_dataset.strftime('%Y-%m-%d'),\n",
    "        \"target\": ts[\n",
    "            start_dataset : end_training - timedelta(days=1)\n",
    "        ].tolist(),  # We use -1, because pandas indexing includes the upper bound\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7f52b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": start_dataset.strftime('%Y-%m-%d'),\n",
    "        \"target\": ts[start_dataset : end_training + timedelta(days=7*prediction_length)].tolist(),\n",
    "    }\n",
    "    # for k in range(1, num_test_windows + 1)\n",
    "    for ts in timeseries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "44c70fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test data\n",
    "import numpy as np\n",
    "np.save('DeepArTest', test_data[0]['target'][len(training_data[0]['target']):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d8ac831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa9de22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 ms, sys: 0 ns, total: 2.56 ms\n",
      "Wall time: 2.07 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ababb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=True):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e013f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-533267229950/deepar-gas-prices/data/train/train.json\n",
      "Overwriting existing file\n",
      "Uploading file to s3://sagemaker-us-east-2-533267229950/deepar-gas-prices/data/test/test.json\n",
      "CPU times: user 31.7 ms, sys: 2.69 ms, total: 34.4 ms\n",
      "Wall time: 145 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9367a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2015-07-30\", \"target\": [0.0, 335.32142857142856, 171.52714285714288, 147.7357142857143, 6...\n"
     ]
    }
   ],
   "source": [
    "s3_sample = s3.Object(s3_bucket, s3_prefix + \"/data/train/train.json\").get()[\"Body\"].read()\n",
    "StringVariable = s3_sample.decode(\"UTF-8\", \"ignore\")\n",
    "lines = StringVariable.split(\"\\n\")\n",
    "print(lines[0][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eda1e517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.2xlarge\",\n",
    "    base_job_name=\"deepar-gas-prices\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "71407a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"200\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "109687f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ca99fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: deepar-gas-prices-2024-04-10-21-11-53-938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-10 21:11:54 Starting - Starting the training job...\n",
      "2024-04-10 21:12:09 Starting - Preparing the instances for training......\n",
      "2024-04-10 21:13:10 Downloading - Downloading input data...\n",
      "2024-04-10 21:13:35 Downloading - Downloading the training image...............\n",
      "2024-04-10 21:16:21 Training - Training image download completed. Training in progress...\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '100', 'early_stopping_patience': '40', 'epochs': '200', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '153', 'time_freq': '7D'}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '100', 'epochs': '200', 'prediction_length': '153', 'time_freq': '7D'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] random_seed is None\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Real time series\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] number of time series: 1\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] number of observations: 300\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] mean target length: 300.0\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] min/mean/max target: 0.0/40.868701171875/335.3214416503906\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] mean abs(target): 40.868701171875\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Small number of time series. Doing 640 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Real time series\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] number of time series: 1\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] number of observations: 453\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] mean target length: 453.0\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] min/mean/max target: 0.0/44.30367860099338/335.3214416503906\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] mean abs(target): 44.30367860099338\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] contains missing values: no\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] #memory_usage::<batchbuffer> = 7.63671875 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:39 INFO 139627334805312] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783799.5024147, \"EndTime\": 1712783800.092211, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 588.8090133666992, \"count\": 1, \"min\": 588.8090133666992, \"max\": 588.8090133666992}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:40 INFO 139627334805312] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:41 INFO 139627334805312] #memory_usage::<model> = 110 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783800.092303, \"EndTime\": 1712783801.1251879, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 1622.633695602417, \"count\": 1, \"min\": 1622.633695602417, \"max\": 1622.633695602417}}}\u001b[0m\n",
      "\u001b[34m[21:16:41] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.406.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 10240 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:42 INFO 139627334805312] Epoch[0] Batch[0] avg_epoch_loss=4.056250\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.056249618530273\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:44 INFO 139627334805312] Epoch[0] Batch[5] avg_epoch_loss=3.931170\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.931170185407003\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:44 INFO 139627334805312] Epoch[0] Batch [5]#011Speed: 113.85 samples/sec#011loss=3.931170\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] Epoch[0] Batch[10] avg_epoch_loss=3.834974\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=3.7195374965667725\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] Epoch[0] Batch [10]#011Speed: 111.41 samples/sec#011loss=3.719537\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783801.1252625, \"EndTime\": 1712783807.7955828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 6670.242786407471, \"count\": 1, \"min\": 6670.242786407471, \"max\": 6670.242786407471}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=103.44284296972168 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.8349735086614434\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:47 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_6bfc83a3-1704-4a4b-bf51-8cdf918701a8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783807.795659, \"EndTime\": 1712783807.8770983, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 81.04872703552246, \"count\": 1, \"min\": 81.04872703552246, \"max\": 81.04872703552246}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:48 INFO 139627334805312] Epoch[1] Batch[0] avg_epoch_loss=3.649209\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.6492090225219727\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:16:51 INFO 139627334805312] Epoch[1] Batch[5] avg_epoch_loss=3.551490\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.5514903465906777\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:51 INFO 139627334805312] Epoch[1] Batch [5]#011Speed: 117.82 samples/sec#011loss=3.551490\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:53 INFO 139627334805312] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783807.8771653, \"EndTime\": 1712783813.4419286, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5564.700365066528, \"count\": 1, \"min\": 5564.700365066528, \"max\": 5564.700365066528}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:53 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.41445763744332 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:53 INFO 139627334805312] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.4534059286117555\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:53 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:53 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_44b3b388-40e3-48ff-8a8d-ebba32d6330a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783813.4420016, \"EndTime\": 1712783813.5227568, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 80.2772045135498, \"count\": 1, \"min\": 80.2772045135498, \"max\": 80.2772045135498}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:54 INFO 139627334805312] Epoch[2] Batch[0] avg_epoch_loss=3.379827\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.3798274993896484\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:56 INFO 139627334805312] Epoch[2] Batch[5] avg_epoch_loss=3.285459\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.2854590813318887\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:56 INFO 139627334805312] Epoch[2] Batch [5]#011Speed: 119.71 samples/sec#011loss=3.285459\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:59 INFO 139627334805312] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783813.5228224, \"EndTime\": 1712783819.0348752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5511.993169784546, \"count\": 1, \"min\": 5511.993169784546, \"max\": 5511.993169784546}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:59 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.0197689335098 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:59 INFO 139627334805312] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:59 INFO 139627334805312] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.2391887664794923\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:59 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:59 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_2cbc4ecc-b9eb-44dc-a283-48ef5a80f907-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783819.0349452, \"EndTime\": 1712783819.114594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 79.23626899719238, \"count\": 1, \"min\": 79.23626899719238, \"max\": 79.23626899719238}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:59 INFO 139627334805312] Epoch[3] Batch[0] avg_epoch_loss=3.121712\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:16:59 INFO 139627334805312] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.1217124462127686\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:02 INFO 139627334805312] Epoch[3] Batch[5] avg_epoch_loss=3.112898\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.1128979921340942\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:02 INFO 139627334805312] Epoch[3] Batch [5]#011Speed: 107.66 samples/sec#011loss=3.112898\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:05 INFO 139627334805312] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783819.1146603, \"EndTime\": 1712783825.1455173, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6030.799627304077, \"count\": 1, \"min\": 6030.799627304077, \"max\": 6030.799627304077}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:05 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.95433933572262 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:05 INFO 139627334805312] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.1039002656936647\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:05 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:05 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_0178e060-d02b-4cdd-a6ff-9ecb3c8d31e8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783825.145586, \"EndTime\": 1712783825.2236052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.58951187133789, \"count\": 1, \"min\": 77.58951187133789, \"max\": 77.58951187133789}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:05 INFO 139627334805312] Epoch[4] Batch[0] avg_epoch_loss=2.988235\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=2.988234758377075\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:08 INFO 139627334805312] Epoch[4] Batch[5] avg_epoch_loss=3.009594\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.009593963623047\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:08 INFO 139627334805312] Epoch[4] Batch [5]#011Speed: 119.52 samples/sec#011loss=3.009594\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] Epoch[4] Batch[10] avg_epoch_loss=2.930708\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=2.8360455989837647\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] Epoch[4] Batch [10]#011Speed: 118.35 samples/sec#011loss=2.836046\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783825.2236698, \"EndTime\": 1712783831.3258996, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6102.174997329712, \"count\": 1, \"min\": 6102.174997329712, \"max\": 6102.174997329712}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.97585065513181 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=4, train loss <loss>=2.930708343332464\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:11 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_7d27eeb6-7273-4940-ab72-7f294329cf09-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783831.3259647, \"EndTime\": 1712783831.4039931, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.70085334777832, \"count\": 1, \"min\": 77.70085334777832, \"max\": 77.70085334777832}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:12 INFO 139627334805312] Epoch[5] Batch[0] avg_epoch_loss=2.927582\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=2.927582263946533\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:14 INFO 139627334805312] Epoch[5] Batch[5] avg_epoch_loss=2.907683\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=2.9076828559239707\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:14 INFO 139627334805312] Epoch[5] Batch [5]#011Speed: 120.00 samples/sec#011loss=2.907683\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:16 INFO 139627334805312] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783831.4040587, \"EndTime\": 1712783836.9620912, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5557.973384857178, \"count\": 1, \"min\": 5557.973384857178, \"max\": 5557.973384857178}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:16 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.42802932292486 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:16 INFO 139627334805312] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=5, train loss <loss>=2.898194098472595\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:16 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:17 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_67bd926a-3803-4823-8460-c97e3608693e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783836.962164, \"EndTime\": 1712783837.0419157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 79.2703628540039, \"count\": 1, \"min\": 79.2703628540039, \"max\": 79.2703628540039}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:17 INFO 139627334805312] Epoch[6] Batch[0] avg_epoch_loss=2.987708\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=2.9877078533172607\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:20 INFO 139627334805312] Epoch[6] Batch[5] avg_epoch_loss=2.914011\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=2.9140106439590454\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:20 INFO 139627334805312] Epoch[6] Batch [5]#011Speed: 121.17 samples/sec#011loss=2.914011\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] Epoch[6] Batch[10] avg_epoch_loss=2.904411\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=2.892891216278076\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] Epoch[6] Batch [10]#011Speed: 119.09 samples/sec#011loss=2.892891\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783837.0419817, \"EndTime\": 1712783843.0843647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6042.324542999268, \"count\": 1, \"min\": 6042.324542999268, \"max\": 6042.324542999268}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.41426863372038 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=6, train loss <loss>=2.9044109041040596\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] Epoch[7] Batch[0] avg_epoch_loss=2.774934\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=2.7749335765838623\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:17:26 INFO 139627334805312] Epoch[7] Batch[5] avg_epoch_loss=2.845819\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=2.845818877220154\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:26 INFO 139627334805312] Epoch[7] Batch [5]#011Speed: 120.19 samples/sec#011loss=2.845819\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:28 INFO 139627334805312] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783843.0844316, \"EndTime\": 1712783848.5861664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5501.422882080078, \"count\": 1, \"min\": 5501.422882080078, \"max\": 5501.422882080078}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:28 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=116.33134209395453 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:28 INFO 139627334805312] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:28 INFO 139627334805312] #quality_metric: host=algo-1, epoch=7, train loss <loss>=2.844645071029663\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:28 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:28 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_913cba1d-d27e-4bbb-8ebc-64db95cbe963-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783848.586238, \"EndTime\": 1712783848.6640878, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.46076583862305, \"count\": 1, \"min\": 77.46076583862305, \"max\": 77.46076583862305}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:29 INFO 139627334805312] Epoch[8] Batch[0] avg_epoch_loss=2.807840\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=2.8078396320343018\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:32 INFO 139627334805312] Epoch[8] Batch[5] avg_epoch_loss=2.764536\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=2.764535665512085\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:32 INFO 139627334805312] Epoch[8] Batch [5]#011Speed: 120.28 samples/sec#011loss=2.764536\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] Epoch[8] Batch[10] avg_epoch_loss=2.727066\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=2.682102108001709\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] Epoch[8] Batch [10]#011Speed: 119.23 samples/sec#011loss=2.682102\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783848.6641517, \"EndTime\": 1712783854.7049189, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6040.709972381592, \"count\": 1, \"min\": 6040.709972381592, \"max\": 6040.709972381592}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.24338409916759 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=8, train loss <loss>=2.7270658666437324\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:34 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_525095f0-b777-47e5-9849-ba5ab8f5df82-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783854.704987, \"EndTime\": 1712783854.7829945, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.67987251281738, \"count\": 1, \"min\": 77.67987251281738, \"max\": 77.67987251281738}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:35 INFO 139627334805312] Epoch[9] Batch[0] avg_epoch_loss=2.815407\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=2.8154070377349854\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:38 INFO 139627334805312] Epoch[9] Batch[5] avg_epoch_loss=2.749992\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=2.7499923706054688\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:38 INFO 139627334805312] Epoch[9] Batch [5]#011Speed: 119.85 samples/sec#011loss=2.749992\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] Epoch[9] Batch[10] avg_epoch_loss=2.720837\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=2.6858498573303224\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] Epoch[9] Batch [10]#011Speed: 118.63 samples/sec#011loss=2.685850\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783854.7830613, \"EndTime\": 1712783860.8477557, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6064.637660980225, \"count\": 1, \"min\": 6064.637660980225, \"max\": 6064.637660980225}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.66100304925811 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=9, train loss <loss>=2.7208366827531294\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:40 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_5fc82615-3697-4b73-a4fb-fdf8f390ce37-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783860.84782, \"EndTime\": 1712783860.9268734, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.64904403686523, \"count\": 1, \"min\": 78.64904403686523, \"max\": 78.64904403686523}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:41 INFO 139627334805312] Epoch[10] Batch[0] avg_epoch_loss=2.748755\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=2.7487552165985107\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:44 INFO 139627334805312] Epoch[10] Batch[5] avg_epoch_loss=2.711913\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=2.7119131882985434\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:44 INFO 139627334805312] Epoch[10] Batch [5]#011Speed: 117.95 samples/sec#011loss=2.711913\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:46 INFO 139627334805312] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783860.9269423, \"EndTime\": 1712783866.482989, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5555.987358093262, \"count\": 1, \"min\": 5555.987358093262, \"max\": 5555.987358093262}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:46 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.60927454031543 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:46 INFO 139627334805312] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=10, train loss <loss>=2.735203218460083\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:46 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:47 INFO 139627334805312] Epoch[11] Batch[0] avg_epoch_loss=2.654277\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=2.6542773246765137\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:49 INFO 139627334805312] Epoch[11] Batch[5] avg_epoch_loss=2.679374\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.6793742974599204\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:49 INFO 139627334805312] Epoch[11] Batch [5]#011Speed: 119.84 samples/sec#011loss=2.679374\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:52 INFO 139627334805312] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783866.4830592, \"EndTime\": 1712783872.0115113, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5527.886629104614, \"count\": 1, \"min\": 5527.886629104614, \"max\": 5527.886629104614}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:52 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.62391580394335 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:52 INFO 139627334805312] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=11, train loss <loss>=2.6474639415740966\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:52 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:52 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_fc9ee736-a30b-4b63-8bf7-ce0ad9c41bee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783872.0115814, \"EndTime\": 1712783872.0892859, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.34274864196777, \"count\": 1, \"min\": 77.34274864196777, \"max\": 77.34274864196777}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:52 INFO 139627334805312] Epoch[12] Batch[0] avg_epoch_loss=2.608316\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.608315944671631\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:17:55 INFO 139627334805312] Epoch[12] Batch[5] avg_epoch_loss=2.640738\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.640738288561503\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:55 INFO 139627334805312] Epoch[12] Batch [5]#011Speed: 120.12 samples/sec#011loss=2.640738\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] Epoch[12] Batch[10] avg_epoch_loss=2.607868\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=2.568423128128052\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] Epoch[12] Batch [10]#011Speed: 119.58 samples/sec#011loss=2.568423\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783872.0893514, \"EndTime\": 1712783878.1331072, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6043.699264526367, \"count\": 1, \"min\": 6043.699264526367, \"max\": 6043.699264526367}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.87926597546847 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.6078677610917524\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_dbad3005-c411-4879-917c-df5b7ef5fa68-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783878.1331704, \"EndTime\": 1712783878.2104373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 76.94721221923828, \"count\": 1, \"min\": 76.94721221923828, \"max\": 76.94721221923828}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] Epoch[13] Batch[0] avg_epoch_loss=2.564152\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:17:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.564152479171753\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:01 INFO 139627334805312] Epoch[13] Batch[5] avg_epoch_loss=2.625595\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.6255950927734375\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:01 INFO 139627334805312] Epoch[13] Batch [5]#011Speed: 117.48 samples/sec#011loss=2.625595\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:04 INFO 139627334805312] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783878.210503, \"EndTime\": 1712783884.2881594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6077.600002288818, \"count\": 1, \"min\": 6077.600002288818, \"max\": 6077.600002288818}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:04 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.30295746895169 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:04 INFO 139627334805312] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.6106542110443116\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:04 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:05 INFO 139627334805312] Epoch[14] Batch[0] avg_epoch_loss=2.522342\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.5223419666290283\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:07 INFO 139627334805312] Epoch[14] Batch[5] avg_epoch_loss=2.581107\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.581106702486674\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:07 INFO 139627334805312] Epoch[14] Batch [5]#011Speed: 117.66 samples/sec#011loss=2.581107\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:09 INFO 139627334805312] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783884.2882318, \"EndTime\": 1712783889.8816292, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5592.901706695557, \"count\": 1, \"min\": 5592.901706695557, \"max\": 5592.901706695557}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:09 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.56789483984477 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:09 INFO 139627334805312] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.5719942092895507\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:09 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:09 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_f602e8c6-5b8f-49b1-b286-0e78a3a99076-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783889.8816996, \"EndTime\": 1712783889.9596612, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.48746871948242, \"count\": 1, \"min\": 77.48746871948242, \"max\": 77.48746871948242}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:10 INFO 139627334805312] Epoch[15] Batch[0] avg_epoch_loss=2.678584\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.6785836219787598\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:13 INFO 139627334805312] Epoch[15] Batch[5] avg_epoch_loss=2.585642\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.585641543070475\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:13 INFO 139627334805312] Epoch[15] Batch [5]#011Speed: 118.33 samples/sec#011loss=2.585642\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:15 INFO 139627334805312] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783889.959727, \"EndTime\": 1712783895.5221207, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5562.335252761841, \"count\": 1, \"min\": 5562.335252761841, \"max\": 5562.335252761841}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:15 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.51811540392153 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:15 INFO 139627334805312] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=15, train loss <loss>=2.5723401069641114\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:15 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:16 INFO 139627334805312] Epoch[16] Batch[0] avg_epoch_loss=2.551799\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=2.5517985820770264\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:18 INFO 139627334805312] Epoch[16] Batch[5] avg_epoch_loss=2.547797\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.5477965672810874\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:18 INFO 139627334805312] Epoch[16] Batch [5]#011Speed: 119.98 samples/sec#011loss=2.547797\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:21 INFO 139627334805312] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783895.5221906, \"EndTime\": 1712783901.0311582, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5508.595943450928, \"count\": 1, \"min\": 5508.595943450928, \"max\": 5508.595943450928}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:21 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.81996956523997 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:21 INFO 139627334805312] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.506530261039734\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:21 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:21 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_abb9ede5-4f80-478f-b571-fa399c3df5fc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783901.031232, \"EndTime\": 1712783901.109823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.13048362731934, \"count\": 1, \"min\": 78.13048362731934, \"max\": 78.13048362731934}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:21 INFO 139627334805312] Epoch[17] Batch[0] avg_epoch_loss=2.570041\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=2.570040702819824\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:18:24 INFO 139627334805312] Epoch[17] Batch[5] avg_epoch_loss=2.545647\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=2.545646627744039\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:24 INFO 139627334805312] Epoch[17] Batch [5]#011Speed: 120.04 samples/sec#011loss=2.545647\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:26 INFO 139627334805312] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783901.1098983, \"EndTime\": 1712783906.6125183, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5502.562522888184, \"count\": 1, \"min\": 5502.562522888184, \"max\": 5502.562522888184}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:26 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.58149475715058 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:26 INFO 139627334805312] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.5579773664474486\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:26 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:27 INFO 139627334805312] Epoch[18] Batch[0] avg_epoch_loss=2.524820\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.52482008934021\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:29 INFO 139627334805312] Epoch[18] Batch[5] avg_epoch_loss=2.526127\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.5261269410451255\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:29 INFO 139627334805312] Epoch[18] Batch [5]#011Speed: 120.62 samples/sec#011loss=2.526127\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] Epoch[18] Batch[10] avg_epoch_loss=2.502756\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=2.474711275100708\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] Epoch[18] Batch [10]#011Speed: 119.33 samples/sec#011loss=2.474711\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783906.612582, \"EndTime\": 1712783912.6442206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6031.158685684204, \"count\": 1, \"min\": 6031.158685684204, \"max\": 6031.158685684204}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.41448110918878 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.502756183797663\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:32 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_7e064a64-8628-453c-8456-a3472617fa2e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783912.6442833, \"EndTime\": 1712783912.7218552, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.25358009338379, \"count\": 1, \"min\": 77.25358009338379, \"max\": 77.25358009338379}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:33 INFO 139627334805312] Epoch[19] Batch[0] avg_epoch_loss=2.482136\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.482135534286499\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:36 INFO 139627334805312] Epoch[19] Batch[5] avg_epoch_loss=2.485485\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=2.4854851961135864\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:36 INFO 139627334805312] Epoch[19] Batch [5]#011Speed: 120.33 samples/sec#011loss=2.485485\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] Epoch[19] Batch[10] avg_epoch_loss=2.460027\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=2.4294764518737795\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] Epoch[19] Batch [10]#011Speed: 119.19 samples/sec#011loss=2.429476\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783912.721918, \"EndTime\": 1712783918.7698736, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6047.899484634399, \"count\": 1, \"min\": 6047.899484634399, \"max\": 6047.899484634399}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.7884186142094 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=19, train loss <loss>=2.460026676004583\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:38 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_bb4f301f-d92b-4d21-807e-64b51ce65033-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783918.7699401, \"EndTime\": 1712783918.8486867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.42469215393066, \"count\": 1, \"min\": 78.42469215393066, \"max\": 78.42469215393066}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:39 INFO 139627334805312] Epoch[20] Batch[0] avg_epoch_loss=2.345888\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.345888376235962\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:42 INFO 139627334805312] Epoch[20] Batch[5] avg_epoch_loss=2.426444\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=2.4264442125956216\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:42 INFO 139627334805312] Epoch[20] Batch [5]#011Speed: 119.59 samples/sec#011loss=2.426444\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:44 INFO 139627334805312] Epoch[20] Batch[10] avg_epoch_loss=2.474668\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=2.5325369358062746\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:44 INFO 139627334805312] Epoch[20] Batch [10]#011Speed: 120.58 samples/sec#011loss=2.532537\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:44 INFO 139627334805312] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783918.8487532, \"EndTime\": 1712783924.8844378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6035.625696182251, \"count\": 1, \"min\": 6035.625696182251, \"max\": 6035.625696182251}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:44 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.34864839754954 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:44 INFO 139627334805312] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=20, train loss <loss>=2.474668177691373\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:44 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:45 INFO 139627334805312] Epoch[21] Batch[0] avg_epoch_loss=2.470127\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.4701268672943115\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:48 INFO 139627334805312] Epoch[21] Batch[5] avg_epoch_loss=2.469325\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.469325304031372\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:48 INFO 139627334805312] Epoch[21] Batch [5]#011Speed: 120.23 samples/sec#011loss=2.469325\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:50 INFO 139627334805312] Epoch[21] Batch[10] avg_epoch_loss=2.446998\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=2.420205020904541\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:50 INFO 139627334805312] Epoch[21] Batch [10]#011Speed: 119.66 samples/sec#011loss=2.420205\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:50 INFO 139627334805312] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783924.8845208, \"EndTime\": 1712783930.9395301, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6054.65841293335, \"count\": 1, \"min\": 6054.65841293335, \"max\": 6054.65841293335}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:50 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.3264690313356 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:50 INFO 139627334805312] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.4469979026100854\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:50 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:51 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_f5011aef-2b2e-43da-8da2-08b8ac33bbed-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783930.9395978, \"EndTime\": 1712783931.022464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 82.53741264343262, \"count\": 1, \"min\": 82.53741264343262, \"max\": 82.53741264343262}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:51 INFO 139627334805312] Epoch[22] Batch[0] avg_epoch_loss=2.416858\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=2.416857957839966\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:18:54 INFO 139627334805312] Epoch[22] Batch[5] avg_epoch_loss=2.475248\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=2.4752482573191323\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:54 INFO 139627334805312] Epoch[22] Batch [5]#011Speed: 116.30 samples/sec#011loss=2.475248\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:56 INFO 139627334805312] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783931.0225317, \"EndTime\": 1712783936.6748931, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5652.296304702759, \"count\": 1, \"min\": 5652.296304702759, \"max\": 5652.296304702759}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:56 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.5724205613985 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:56 INFO 139627334805312] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=22, train loss <loss>=2.4660670280456545\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:56 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:57 INFO 139627334805312] Epoch[23] Batch[0] avg_epoch_loss=2.524854\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:18:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.5248541831970215\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:00 INFO 139627334805312] Epoch[23] Batch[5] avg_epoch_loss=2.446774\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.4467740058898926\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:00 INFO 139627334805312] Epoch[23] Batch [5]#011Speed: 120.61 samples/sec#011loss=2.446774\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:02 INFO 139627334805312] Epoch[23] Batch[10] avg_epoch_loss=2.410496\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=2.366963291168213\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:02 INFO 139627334805312] Epoch[23] Batch [10]#011Speed: 109.10 samples/sec#011loss=2.366963\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:02 INFO 139627334805312] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783936.6749659, \"EndTime\": 1712783942.9651139, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6289.764165878296, \"count\": 1, \"min\": 6289.764165878296, \"max\": 6289.764165878296}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:02 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.7434452684139 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:02 INFO 139627334805312] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=23, train loss <loss>=2.410496408289129\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:02 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:03 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_a31d28f8-6620-442e-805c-9852a65b057e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783942.96535, \"EndTime\": 1712783943.0453777, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 79.27942276000977, \"count\": 1, \"min\": 79.27942276000977, \"max\": 79.27942276000977}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:03 INFO 139627334805312] Epoch[24] Batch[0] avg_epoch_loss=2.476281\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.47628116607666\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:06 INFO 139627334805312] Epoch[24] Batch[5] avg_epoch_loss=2.433065\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:06 INFO 139627334805312] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=2.4330646991729736\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:06 INFO 139627334805312] Epoch[24] Batch [5]#011Speed: 115.78 samples/sec#011loss=2.433065\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:08 INFO 139627334805312] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783943.0454414, \"EndTime\": 1712783948.7026987, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5657.201528549194, \"count\": 1, \"min\": 5657.201528549194, \"max\": 5657.201528549194}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:08 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.1280897241805 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:08 INFO 139627334805312] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=24, train loss <loss>=2.4125718593597414\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:08 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:09 INFO 139627334805312] Epoch[25] Batch[0] avg_epoch_loss=2.375316\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.3753163814544678\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:12 INFO 139627334805312] Epoch[25] Batch[5] avg_epoch_loss=2.410869\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.4108692407608032\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:12 INFO 139627334805312] Epoch[25] Batch [5]#011Speed: 120.39 samples/sec#011loss=2.410869\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:14 INFO 139627334805312] Epoch[25] Batch[10] avg_epoch_loss=2.428733\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=2.4501688480377197\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:14 INFO 139627334805312] Epoch[25] Batch [10]#011Speed: 120.20 samples/sec#011loss=2.450169\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:14 INFO 139627334805312] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783948.7027688, \"EndTime\": 1712783954.7211585, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6018.00799369812, \"count\": 1, \"min\": 6018.00799369812, \"max\": 6018.00799369812}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:14 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.1546939245735 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:14 INFO 139627334805312] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=25, train loss <loss>=2.4287326986139472\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:14 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:15 INFO 139627334805312] Epoch[26] Batch[0] avg_epoch_loss=2.321253\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.3212528228759766\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:18 INFO 139627334805312] Epoch[26] Batch[5] avg_epoch_loss=2.397274\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.397274454434713\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:18 INFO 139627334805312] Epoch[26] Batch [5]#011Speed: 119.89 samples/sec#011loss=2.397274\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:20 INFO 139627334805312] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783954.7212682, \"EndTime\": 1712783960.2058957, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5484.316349029541, \"count\": 1, \"min\": 5484.316349029541, \"max\": 5484.316349029541}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:20 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.1301540594742 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:20 INFO 139627334805312] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.3989044666290282\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:20 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:20 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_185ab029-5ede-488e-9ed4-47a9713d6f55-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783960.205967, \"EndTime\": 1712783960.283804, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.37588882446289, \"count\": 1, \"min\": 77.37588882446289, \"max\": 77.37588882446289}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:20 INFO 139627334805312] Epoch[27] Batch[0] avg_epoch_loss=2.265879\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.265878677368164\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:19:23 INFO 139627334805312] Epoch[27] Batch[5] avg_epoch_loss=2.386025\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=2.3860251108805337\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:23 INFO 139627334805312] Epoch[27] Batch [5]#011Speed: 119.89 samples/sec#011loss=2.386025\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:26 INFO 139627334805312] Epoch[27] Batch[10] avg_epoch_loss=2.408148\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=2.434694766998291\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:26 INFO 139627334805312] Epoch[27] Batch [10]#011Speed: 120.05 samples/sec#011loss=2.434695\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:26 INFO 139627334805312] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783960.2838688, \"EndTime\": 1712783966.3190606, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6035.134077072144, \"count\": 1, \"min\": 6035.134077072144, \"max\": 6035.134077072144}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:26 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.86078985310382 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:26 INFO 139627334805312] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=27, train loss <loss>=2.408147681843151\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:26 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:27 INFO 139627334805312] Epoch[28] Batch[0] avg_epoch_loss=2.372178\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.372178077697754\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:29 INFO 139627334805312] Epoch[28] Batch[5] avg_epoch_loss=2.352518\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.3525176445643106\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:29 INFO 139627334805312] Epoch[28] Batch [5]#011Speed: 121.34 samples/sec#011loss=2.352518\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] Epoch[28] Batch[10] avg_epoch_loss=2.314235\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=2.268294835090637\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] Epoch[28] Batch [10]#011Speed: 120.42 samples/sec#011loss=2.268295\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783966.3191261, \"EndTime\": 1712783972.3230085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6003.537178039551, \"count\": 1, \"min\": 6003.537178039551, \"max\": 6003.537178039551}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.43437705188988 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.3142345493490044\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:32 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_58387dc9-2f7d-4482-ac87-83cec079a1e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783972.3230739, \"EndTime\": 1712783972.4003358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 76.94101333618164, \"count\": 1, \"min\": 76.94101333618164, \"max\": 76.94101333618164}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:33 INFO 139627334805312] Epoch[29] Batch[0] avg_epoch_loss=2.486927\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.486927032470703\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:35 INFO 139627334805312] Epoch[29] Batch[5] avg_epoch_loss=2.384460\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.384459892908732\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:35 INFO 139627334805312] Epoch[29] Batch [5]#011Speed: 120.93 samples/sec#011loss=2.384460\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:38 INFO 139627334805312] Epoch[29] Batch[10] avg_epoch_loss=2.318363\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=2.2390463590621947\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:38 INFO 139627334805312] Epoch[29] Batch [10]#011Speed: 119.98 samples/sec#011loss=2.239046\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:38 INFO 139627334805312] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783972.4004016, \"EndTime\": 1712783978.4196746, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6019.214868545532, \"count\": 1, \"min\": 6019.214868545532, \"max\": 6019.214868545532}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:38 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.3180587873711 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:38 INFO 139627334805312] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.318362832069397\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:38 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:39 INFO 139627334805312] Epoch[30] Batch[0] avg_epoch_loss=2.390946\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.39094614982605\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:41 INFO 139627334805312] Epoch[30] Batch[5] avg_epoch_loss=2.341760\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.341760436693827\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:41 INFO 139627334805312] Epoch[30] Batch [5]#011Speed: 120.60 samples/sec#011loss=2.341760\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] Epoch[30] Batch[10] avg_epoch_loss=2.301337\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=2.2528290271759035\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] Epoch[30] Batch [10]#011Speed: 119.66 samples/sec#011loss=2.252829\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783978.4197388, \"EndTime\": 1712783984.4433773, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6023.277044296265, \"count\": 1, \"min\": 6023.277044296265, \"max\": 6023.277044296265}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.23339000191974 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.3013370687311347\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:44 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_fa4f87df-c26d-499c-934e-50ed583de4c4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783984.4434414, \"EndTime\": 1712783984.5219357, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.15122604370117, \"count\": 1, \"min\": 78.15122604370117, \"max\": 78.15122604370117}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:45 INFO 139627334805312] Epoch[31] Batch[0] avg_epoch_loss=2.335474\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.3354742527008057\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:47 INFO 139627334805312] Epoch[31] Batch[5] avg_epoch_loss=2.352618\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.3526179790496826\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:47 INFO 139627334805312] Epoch[31] Batch [5]#011Speed: 120.14 samples/sec#011loss=2.352618\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:50 INFO 139627334805312] Epoch[31] Batch[10] avg_epoch_loss=2.368249\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=2.3870051383972166\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:50 INFO 139627334805312] Epoch[31] Batch [10]#011Speed: 121.02 samples/sec#011loss=2.387005\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:50 INFO 139627334805312] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783984.5220013, \"EndTime\": 1712783990.5313451, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6009.288311004639, \"count\": 1, \"min\": 6009.288311004639, \"max\": 6009.288311004639}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:50 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.1641660764772 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:50 INFO 139627334805312] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.3682485060258345\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:50 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:51 INFO 139627334805312] Epoch[32] Batch[0] avg_epoch_loss=2.310870\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.3108701705932617\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:19:53 INFO 139627334805312] Epoch[32] Batch[5] avg_epoch_loss=2.327003\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.3270031611124673\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:53 INFO 139627334805312] Epoch[32] Batch [5]#011Speed: 120.88 samples/sec#011loss=2.327003\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:55 INFO 139627334805312] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783990.53141, \"EndTime\": 1712783995.9989965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5467.291355133057, \"count\": 1, \"min\": 5467.291355133057, \"max\": 5467.291355133057}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:55 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=116.69177368345497 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:55 INFO 139627334805312] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.313420867919922\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:55 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:56 INFO 139627334805312] Epoch[33] Batch[0] avg_epoch_loss=2.333565\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.3335654735565186\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:59 INFO 139627334805312] Epoch[33] Batch[5] avg_epoch_loss=2.330755\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:59 INFO 139627334805312] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.3307551542917886\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:19:59 INFO 139627334805312] Epoch[33] Batch [5]#011Speed: 120.91 samples/sec#011loss=2.330755\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:02 INFO 139627334805312] Epoch[33] Batch[10] avg_epoch_loss=2.317806\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.30226616859436\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:02 INFO 139627334805312] Epoch[33] Batch [10]#011Speed: 110.22 samples/sec#011loss=2.302266\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:02 INFO 139627334805312] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712783995.9990675, \"EndTime\": 1712784002.2596323, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6260.071516036987, \"count\": 1, \"min\": 6260.071516036987, \"max\": 6260.071516036987}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:02 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=102.87266474542962 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:02 INFO 139627334805312] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.317805615338412\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:02 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:03 INFO 139627334805312] Epoch[34] Batch[0] avg_epoch_loss=2.337177\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.33717679977417\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:05 INFO 139627334805312] Epoch[34] Batch[5] avg_epoch_loss=2.310646\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.3106456200281777\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:05 INFO 139627334805312] Epoch[34] Batch [5]#011Speed: 117.84 samples/sec#011loss=2.310646\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:07 INFO 139627334805312] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784002.2596965, \"EndTime\": 1712784007.9749188, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5714.935779571533, \"count\": 1, \"min\": 5714.935779571533, \"max\": 5714.935779571533}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:07 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.51106871846764 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:07 INFO 139627334805312] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.2736690282821654\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:07 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:08 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_12b6fb2a-2d15-4641-9957-db565f3c364e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784007.9749908, \"EndTime\": 1712784008.0525675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.15654373168945, \"count\": 1, \"min\": 77.15654373168945, \"max\": 77.15654373168945}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:08 INFO 139627334805312] Epoch[35] Batch[0] avg_epoch_loss=2.351566\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.3515658378601074\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:11 INFO 139627334805312] Epoch[35] Batch[5] avg_epoch_loss=2.304897\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.304896910985311\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:11 INFO 139627334805312] Epoch[35] Batch [5]#011Speed: 121.13 samples/sec#011loss=2.304897\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:13 INFO 139627334805312] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784008.0526326, \"EndTime\": 1712784013.521169, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5468.478441238403, \"count\": 1, \"min\": 5468.478441238403, \"max\": 5468.478441238403}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:13 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.19188759715338 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:13 INFO 139627334805312] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.2816978454589845\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:13 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:14 INFO 139627334805312] Epoch[36] Batch[0] avg_epoch_loss=2.284361\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.2843613624572754\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:16 INFO 139627334805312] Epoch[36] Batch[5] avg_epoch_loss=2.317259\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.317259152730306\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:16 INFO 139627334805312] Epoch[36] Batch [5]#011Speed: 120.45 samples/sec#011loss=2.317259\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:19 INFO 139627334805312] Epoch[36] Batch[10] avg_epoch_loss=2.342551\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=2.372902011871338\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:19 INFO 139627334805312] Epoch[36] Batch [10]#011Speed: 121.10 samples/sec#011loss=2.372902\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:19 INFO 139627334805312] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784013.5212462, \"EndTime\": 1712784019.5263343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6004.621744155884, \"count\": 1, \"min\": 6004.621744155884, \"max\": 6004.621744155884}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:19 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.41188721691456 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:19 INFO 139627334805312] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.342551361430775\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:19 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:20 INFO 139627334805312] Epoch[37] Batch[0] avg_epoch_loss=2.507639\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.507638931274414\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:22 INFO 139627334805312] Epoch[37] Batch[5] avg_epoch_loss=2.381614\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:22 INFO 139627334805312] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.3816142876942954\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:22 INFO 139627334805312] Epoch[37] Batch [5]#011Speed: 119.68 samples/sec#011loss=2.381614\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:25 INFO 139627334805312] Epoch[37] Batch[10] avg_epoch_loss=2.339582\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=2.2891435623168945\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:25 INFO 139627334805312] Epoch[37] Batch [10]#011Speed: 120.64 samples/sec#011loss=2.289144\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:25 INFO 139627334805312] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784019.5264263, \"EndTime\": 1712784025.551809, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6025.065183639526, \"count\": 1, \"min\": 6025.065183639526, \"max\": 6025.065183639526}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:25 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.20030348971625 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:25 INFO 139627334805312] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.3395821397954766\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:25 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:26 INFO 139627334805312] Epoch[38] Batch[0] avg_epoch_loss=2.304223\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.304222822189331\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:20:28 INFO 139627334805312] Epoch[38] Batch[5] avg_epoch_loss=2.290017\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:28 INFO 139627334805312] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.290016849835714\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:28 INFO 139627334805312] Epoch[38] Batch [5]#011Speed: 120.55 samples/sec#011loss=2.290017\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:31 INFO 139627334805312] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784025.551877, \"EndTime\": 1712784031.0388722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5486.7002964019775, \"count\": 1, \"min\": 5486.7002964019775, \"max\": 5486.7002964019775}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:31 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.98410501200784 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:31 INFO 139627334805312] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.3243141651153563\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:31 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:31 INFO 139627334805312] Epoch[39] Batch[0] avg_epoch_loss=2.260968\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.2609684467315674\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:34 INFO 139627334805312] Epoch[39] Batch[5] avg_epoch_loss=2.222873\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.2228726943333945\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:34 INFO 139627334805312] Epoch[39] Batch [5]#011Speed: 118.60 samples/sec#011loss=2.222873\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] Epoch[39] Batch[10] avg_epoch_loss=2.214721\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=2.204938006401062\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] Epoch[39] Batch [10]#011Speed: 118.87 samples/sec#011loss=2.204938\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784031.0389373, \"EndTime\": 1712784037.1373346, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6097.9626178741455, \"count\": 1, \"min\": 6097.9626178741455, \"max\": 6097.9626178741455}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.7391213369818 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=39, train loss <loss>=2.2147205634550615\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_7b2c350f-90f3-485c-a720-335abddff1cf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784037.1374025, \"EndTime\": 1712784037.2152867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.35347747802734, \"count\": 1, \"min\": 77.35347747802734, \"max\": 77.35347747802734}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] Epoch[40] Batch[0] avg_epoch_loss=2.358544\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.358543634414673\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:40 INFO 139627334805312] Epoch[40] Batch[5] avg_epoch_loss=2.279904\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.279904007911682\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:40 INFO 139627334805312] Epoch[40] Batch [5]#011Speed: 120.40 samples/sec#011loss=2.279904\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:42 INFO 139627334805312] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784037.2153635, \"EndTime\": 1712784042.7267134, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5511.287450790405, \"count\": 1, \"min\": 5511.287450790405, \"max\": 5511.287450790405}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:42 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.21867191262156 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:42 INFO 139627334805312] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.2645427227020263\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:42 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:43 INFO 139627334805312] Epoch[41] Batch[0] avg_epoch_loss=2.247598\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.2475979328155518\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:46 INFO 139627334805312] Epoch[41] Batch[5] avg_epoch_loss=2.269865\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.2698654731114707\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:46 INFO 139627334805312] Epoch[41] Batch [5]#011Speed: 120.69 samples/sec#011loss=2.269865\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:48 INFO 139627334805312] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784042.7268496, \"EndTime\": 1712784048.2189803, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5491.762638092041, \"count\": 1, \"min\": 5491.762638092041, \"max\": 5491.762638092041}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:48 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.62260034403968 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:48 INFO 139627334805312] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.2738561391830445\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:48 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:48 INFO 139627334805312] Epoch[42] Batch[0] avg_epoch_loss=2.193010\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.193009853363037\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:51 INFO 139627334805312] Epoch[42] Batch[5] avg_epoch_loss=2.260386\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.2603855530420938\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:51 INFO 139627334805312] Epoch[42] Batch [5]#011Speed: 120.54 samples/sec#011loss=2.260386\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:53 INFO 139627334805312] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784048.2190497, \"EndTime\": 1712784053.7208288, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5501.333713531494, \"count\": 1, \"min\": 5501.333713531494, \"max\": 5501.333713531494}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:53 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.60648050638923 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:53 INFO 139627334805312] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.2472133159637453\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:53 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:54 INFO 139627334805312] Epoch[43] Batch[0] avg_epoch_loss=2.181139\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.181138515472412\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:57 INFO 139627334805312] Epoch[43] Batch[5] avg_epoch_loss=2.238486\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.238486131032308\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:20:57 INFO 139627334805312] Epoch[43] Batch [5]#011Speed: 114.87 samples/sec#011loss=2.238486\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] Epoch[43] Batch[10] avg_epoch_loss=2.227132\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=2.2135072231292723\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] Epoch[43] Batch [10]#011Speed: 114.32 samples/sec#011loss=2.213507\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784053.7209089, \"EndTime\": 1712784060.0089748, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6287.646055221558, \"count\": 1, \"min\": 6287.646055221558, \"max\": 6287.646055221558}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=104.64805340582647 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.2271320819854736\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] Epoch[44] Batch[0] avg_epoch_loss=2.205685\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.2056851387023926\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:21:03 INFO 139627334805312] Epoch[44] Batch[5] avg_epoch_loss=2.253369\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.2533686558405557\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:03 INFO 139627334805312] Epoch[44] Batch [5]#011Speed: 108.20 samples/sec#011loss=2.253369\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:05 INFO 139627334805312] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784060.0090396, \"EndTime\": 1712784065.8251717, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5815.828084945679, \"count\": 1, \"min\": 5815.828084945679, \"max\": 5815.828084945679}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:05 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.60270781948641 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:05 INFO 139627334805312] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.2486093997955323\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:05 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:06 INFO 139627334805312] Epoch[45] Batch[0] avg_epoch_loss=2.191473\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:06 INFO 139627334805312] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.1914734840393066\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:09 INFO 139627334805312] Epoch[45] Batch[5] avg_epoch_loss=2.220330\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.220329840977987\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:09 INFO 139627334805312] Epoch[45] Batch [5]#011Speed: 119.26 samples/sec#011loss=2.220330\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:11 INFO 139627334805312] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784065.8252943, \"EndTime\": 1712784071.3568614, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5531.109809875488, \"count\": 1, \"min\": 5531.109809875488, \"max\": 5531.109809875488}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:11 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.34477695092374 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:11 INFO 139627334805312] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.2326963901519776\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:11 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:12 INFO 139627334805312] Epoch[46] Batch[0] avg_epoch_loss=2.265483\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.2654833793640137\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:14 INFO 139627334805312] Epoch[46] Batch[5] avg_epoch_loss=2.225677\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=2.2256770531336465\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:14 INFO 139627334805312] Epoch[46] Batch [5]#011Speed: 121.51 samples/sec#011loss=2.225677\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] Epoch[46] Batch[10] avg_epoch_loss=2.203162\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=2.1761442184448243\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] Epoch[46] Batch [10]#011Speed: 119.36 samples/sec#011loss=2.176144\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784071.3569546, \"EndTime\": 1712784077.3790436, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6021.577596664429, \"count\": 1, \"min\": 6021.577596664429, \"max\": 6021.577596664429}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.58967327997938 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=46, train loss <loss>=2.203162128275091\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:17 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_701020fe-b13e-4a4a-b3dc-97ba6433f49f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784077.379107, \"EndTime\": 1712784077.457465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.99673080444336, \"count\": 1, \"min\": 77.99673080444336, \"max\": 77.99673080444336}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:18 INFO 139627334805312] Epoch[47] Batch[0] avg_epoch_loss=2.075165\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.075165033340454\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:20 INFO 139627334805312] Epoch[47] Batch[5] avg_epoch_loss=2.207866\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.2078657150268555\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:20 INFO 139627334805312] Epoch[47] Batch [5]#011Speed: 121.21 samples/sec#011loss=2.207866\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] Epoch[47] Batch[10] avg_epoch_loss=2.188354\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=2.164940333366394\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] Epoch[47] Batch [10]#011Speed: 120.81 samples/sec#011loss=2.164940\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784077.4575324, \"EndTime\": 1712784083.4471412, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5989.552021026611, \"count\": 1, \"min\": 5989.552021026611, \"max\": 5989.552021026611}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.69096810667862 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.188354177908464\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:23 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_2e25b560-53f0-4645-9477-dd6a969cb2d1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784083.4472065, \"EndTime\": 1712784083.526119, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.53555679321289, \"count\": 1, \"min\": 78.53555679321289, \"max\": 78.53555679321289}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:24 INFO 139627334805312] Epoch[48] Batch[0] avg_epoch_loss=2.149542\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=2.1495418548583984\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:26 INFO 139627334805312] Epoch[48] Batch[5] avg_epoch_loss=2.223321\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.2233211199442544\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:26 INFO 139627334805312] Epoch[48] Batch [5]#011Speed: 120.54 samples/sec#011loss=2.223321\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:29 INFO 139627334805312] Epoch[48] Batch[10] avg_epoch_loss=2.232344\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=2.2431724071502686\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:29 INFO 139627334805312] Epoch[48] Batch [10]#011Speed: 121.29 samples/sec#011loss=2.243172\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:29 INFO 139627334805312] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784083.526184, \"EndTime\": 1712784089.523609, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5997.369289398193, \"count\": 1, \"min\": 5997.369289398193, \"max\": 5997.369289398193}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:29 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.87931511247218 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:29 INFO 139627334805312] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.2323444323106245\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:29 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:30 INFO 139627334805312] Epoch[49] Batch[0] avg_epoch_loss=2.312635\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.3126347064971924\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:32 INFO 139627334805312] Epoch[49] Batch[5] avg_epoch_loss=2.216953\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.2169525225957236\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:32 INFO 139627334805312] Epoch[49] Batch [5]#011Speed: 120.96 samples/sec#011loss=2.216953\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:35 INFO 139627334805312] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784089.5236754, \"EndTime\": 1712784095.0638783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5539.898633956909, \"count\": 1, \"min\": 5539.898633956909, \"max\": 5539.898633956909}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:35 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.63530712661512 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:35 INFO 139627334805312] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.2095676183700563\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:35 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:35 INFO 139627334805312] Epoch[50] Batch[0] avg_epoch_loss=2.221607\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.221606731414795\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:21:38 INFO 139627334805312] Epoch[50] Batch[5] avg_epoch_loss=2.196604\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=2.1966044902801514\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:38 INFO 139627334805312] Epoch[50] Batch [5]#011Speed: 115.52 samples/sec#011loss=2.196604\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:40 INFO 139627334805312] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784095.06395, \"EndTime\": 1712784100.7934535, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5729.0637493133545, \"count\": 1, \"min\": 5729.0637493133545, \"max\": 5729.0637493133545}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:40 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.6944498451262 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:40 INFO 139627334805312] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=50, train loss <loss>=2.1939250946044924\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:40 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:41 INFO 139627334805312] Epoch[51] Batch[0] avg_epoch_loss=2.286691\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.286691188812256\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:44 INFO 139627334805312] Epoch[51] Batch[5] avg_epoch_loss=2.212942\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.212941805521647\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:44 INFO 139627334805312] Epoch[51] Batch [5]#011Speed: 114.88 samples/sec#011loss=2.212942\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:46 INFO 139627334805312] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784100.7935278, \"EndTime\": 1712784106.5880392, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5794.167518615723, \"count\": 1, \"min\": 5794.167518615723, \"max\": 5794.167518615723}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:46 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.86510344341765 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:46 INFO 139627334805312] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.2039008378982543\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:46 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:47 INFO 139627334805312] Epoch[52] Batch[0] avg_epoch_loss=2.138915\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=2.1389153003692627\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:50 INFO 139627334805312] Epoch[52] Batch[5] avg_epoch_loss=2.190053\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.1900533040364585\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:50 INFO 139627334805312] Epoch[52] Batch [5]#011Speed: 116.71 samples/sec#011loss=2.190053\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:52 INFO 139627334805312] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784106.5881133, \"EndTime\": 1712784112.2204323, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5631.906747817993, \"count\": 1, \"min\": 5631.906747817993, \"max\": 5631.906747817993}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:52 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.39348554985041 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:52 INFO 139627334805312] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.1990169525146483\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:52 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:52 INFO 139627334805312] Epoch[53] Batch[0] avg_epoch_loss=2.183002\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.183002471923828\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:55 INFO 139627334805312] Epoch[53] Batch[5] avg_epoch_loss=2.178252\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.1782522201538086\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:55 INFO 139627334805312] Epoch[53] Batch [5]#011Speed: 120.51 samples/sec#011loss=2.178252\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:58 INFO 139627334805312] Epoch[53] Batch[10] avg_epoch_loss=2.223126\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=2.2769737243652344\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:58 INFO 139627334805312] Epoch[53] Batch [10]#011Speed: 118.67 samples/sec#011loss=2.276974\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:58 INFO 139627334805312] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784112.220495, \"EndTime\": 1712784118.2801583, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6059.214115142822, \"count\": 1, \"min\": 6059.214115142822, \"max\": 6059.214115142822}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:58 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.94273579538077 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:58 INFO 139627334805312] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.223125631159002\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:58 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:59 INFO 139627334805312] Epoch[54] Batch[0] avg_epoch_loss=2.142984\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:21:59 INFO 139627334805312] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.1429836750030518\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:01 INFO 139627334805312] Epoch[54] Batch[5] avg_epoch_loss=2.159174\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.159173925717672\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:01 INFO 139627334805312] Epoch[54] Batch [5]#011Speed: 118.27 samples/sec#011loss=2.159174\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:04 INFO 139627334805312] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784118.2802234, \"EndTime\": 1712784124.0449922, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5764.469861984253, \"count\": 1, \"min\": 5764.469861984253, \"max\": 5764.469861984253}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:04 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.50254533266342 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:04 INFO 139627334805312] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.1714518785476686\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:04 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:04 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_43acbf25-becd-4ebe-bd35-fb0206fa8075-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784124.0450635, \"EndTime\": 1712784124.1229248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.47697830200195, \"count\": 1, \"min\": 77.47697830200195, \"max\": 77.47697830200195}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:04 INFO 139627334805312] Epoch[55] Batch[0] avg_epoch_loss=2.129076\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.1290760040283203\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:07 INFO 139627334805312] Epoch[55] Batch[5] avg_epoch_loss=2.177891\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.1778910557428994\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:07 INFO 139627334805312] Epoch[55] Batch [5]#011Speed: 121.20 samples/sec#011loss=2.177891\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] Epoch[55] Batch[10] avg_epoch_loss=2.200262\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=2.2271076679229735\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] Epoch[55] Batch [10]#011Speed: 121.37 samples/sec#011loss=2.227108\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784124.1229916, \"EndTime\": 1712784130.1017296, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5978.681802749634, \"count\": 1, \"min\": 5978.681802749634, \"max\": 5978.681802749634}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.21963223798556 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.2002622430974785\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] Epoch[56] Batch[0] avg_epoch_loss=2.169442\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.1694421768188477\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:13 INFO 139627334805312] Epoch[56] Batch[5] avg_epoch_loss=2.161215\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.161214749018351\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:13 INFO 139627334805312] Epoch[56] Batch [5]#011Speed: 120.67 samples/sec#011loss=2.161215\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:15 INFO 139627334805312] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784130.1017962, \"EndTime\": 1712784135.5625758, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5460.482120513916, \"count\": 1, \"min\": 5460.482120513916, \"max\": 5460.482120513916}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:15 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.90742098380449 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:15 INFO 139627334805312] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.1801422357559206\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:15 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:16 INFO 139627334805312] Epoch[57] Batch[0] avg_epoch_loss=2.136627\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=2.136627435684204\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:22:18 INFO 139627334805312] Epoch[57] Batch[5] avg_epoch_loss=2.195875\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=2.1958752075831094\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:18 INFO 139627334805312] Epoch[57] Batch [5]#011Speed: 120.90 samples/sec#011loss=2.195875\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:21 INFO 139627334805312] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784135.5626395, \"EndTime\": 1712784141.0457041, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5482.341051101685, \"count\": 1, \"min\": 5482.341051101685, \"max\": 5482.341051101685}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:21 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.91216850371389 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:21 INFO 139627334805312] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=57, train loss <loss>=2.182186985015869\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:21 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:21 INFO 139627334805312] Epoch[58] Batch[0] avg_epoch_loss=2.226334\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.2263338565826416\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:24 INFO 139627334805312] Epoch[58] Batch[5] avg_epoch_loss=2.194104\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=2.194104234377543\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:24 INFO 139627334805312] Epoch[58] Batch [5]#011Speed: 120.40 samples/sec#011loss=2.194104\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:26 INFO 139627334805312] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784141.0457766, \"EndTime\": 1712784146.5508244, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5504.663944244385, \"count\": 1, \"min\": 5504.663944244385, \"max\": 5504.663944244385}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:26 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.6331186046002 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:26 INFO 139627334805312] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=58, train loss <loss>=2.1811234951019287\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:26 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:27 INFO 139627334805312] Epoch[59] Batch[0] avg_epoch_loss=2.109563\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=2.1095633506774902\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:29 INFO 139627334805312] Epoch[59] Batch[5] avg_epoch_loss=2.143866\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=2.1438656648000083\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:29 INFO 139627334805312] Epoch[59] Batch [5]#011Speed: 119.43 samples/sec#011loss=2.143866\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:32 INFO 139627334805312] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784146.5508947, \"EndTime\": 1712784152.0893693, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5538.104295730591, \"count\": 1, \"min\": 5538.104295730591, \"max\": 5538.104295730591}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:32 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.01918375721439 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:32 INFO 139627334805312] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=59, train loss <loss>=2.151896905899048\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:32 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:32 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_71691975-86eb-4efe-b38b-a5ad53d49acb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784152.0894394, \"EndTime\": 1712784152.1679037, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.04083824157715, \"count\": 1, \"min\": 78.04083824157715, \"max\": 78.04083824157715}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:32 INFO 139627334805312] Epoch[60] Batch[0] avg_epoch_loss=2.104449\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=2.1044492721557617\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:35 INFO 139627334805312] Epoch[60] Batch[5] avg_epoch_loss=2.145325\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.1453245878219604\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:35 INFO 139627334805312] Epoch[60] Batch [5]#011Speed: 120.43 samples/sec#011loss=2.145325\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:37 INFO 139627334805312] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784152.167973, \"EndTime\": 1712784157.9170616, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5749.028921127319, \"count\": 1, \"min\": 5749.028921127319, \"max\": 5749.028921127319}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:37 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=103.84172957891217 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:37 INFO 139627334805312] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.1279471158981322\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:37 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:37 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_a5a58e82-753a-4e7d-becb-bf1a1cd6346c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784157.917133, \"EndTime\": 1712784157.996463, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.74226570129395, \"count\": 1, \"min\": 78.74226570129395, \"max\": 78.74226570129395}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:38 INFO 139627334805312] Epoch[61] Batch[0] avg_epoch_loss=2.113142\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.1131415367126465\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:41 INFO 139627334805312] Epoch[61] Batch[5] avg_epoch_loss=2.149073\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.149072527885437\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:41 INFO 139627334805312] Epoch[61] Batch [5]#011Speed: 117.64 samples/sec#011loss=2.149073\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:43 INFO 139627334805312] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784157.996531, \"EndTime\": 1712784163.5771234, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5580.532550811768, \"count\": 1, \"min\": 5580.532550811768, \"max\": 5580.532550811768}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:43 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.96547529381306 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:43 INFO 139627334805312] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.13154022693634\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:43 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:44 INFO 139627334805312] Epoch[62] Batch[0] avg_epoch_loss=2.158823\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.158822774887085\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:47 INFO 139627334805312] Epoch[62] Batch[5] avg_epoch_loss=2.155347\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.155347466468811\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:47 INFO 139627334805312] Epoch[62] Batch [5]#011Speed: 117.28 samples/sec#011loss=2.155347\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] Epoch[62] Batch[10] avg_epoch_loss=2.113110\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=2.0624246120452883\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] Epoch[62] Batch [10]#011Speed: 120.57 samples/sec#011loss=2.062425\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784163.5771952, \"EndTime\": 1712784169.67283, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6095.279216766357, \"count\": 1, \"min\": 6095.279216766357, \"max\": 6095.279216766357}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.32556460063233 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.1131098053672095\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:49 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_6be066f3-57e9-43df-9fda-a80611308faf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784169.6729066, \"EndTime\": 1712784169.7507994, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.55732536315918, \"count\": 1, \"min\": 77.55732536315918, \"max\": 77.55732536315918}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:50 INFO 139627334805312] Epoch[63] Batch[0] avg_epoch_loss=2.194237\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.1942365169525146\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:53 INFO 139627334805312] Epoch[63] Batch[5] avg_epoch_loss=2.146743\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.146742502848307\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:53 INFO 139627334805312] Epoch[63] Batch [5]#011Speed: 119.84 samples/sec#011loss=2.146743\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:55 INFO 139627334805312] Epoch[63] Batch[10] avg_epoch_loss=2.155284\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=2.1655330657958984\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:55 INFO 139627334805312] Epoch[63] Batch [10]#011Speed: 119.43 samples/sec#011loss=2.165533\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:55 INFO 139627334805312] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784169.7508652, \"EndTime\": 1712784175.8191311, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6068.20821762085, \"count\": 1, \"min\": 6068.20821762085, \"max\": 6068.20821762085}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:55 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.4210252821846 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:55 INFO 139627334805312] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.155283667824485\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:55 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:56 INFO 139627334805312] Epoch[64] Batch[0] avg_epoch_loss=2.118256\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.1182563304901123\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:22:59 INFO 139627334805312] Epoch[64] Batch[5] avg_epoch_loss=2.166012\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:59 INFO 139627334805312] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.1660118103027344\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:22:59 INFO 139627334805312] Epoch[64] Batch [5]#011Speed: 119.74 samples/sec#011loss=2.166012\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:01 INFO 139627334805312] Epoch[64] Batch[10] avg_epoch_loss=2.163611\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=2.160730791091919\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:01 INFO 139627334805312] Epoch[64] Batch [10]#011Speed: 118.85 samples/sec#011loss=2.160731\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:01 INFO 139627334805312] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784175.8191948, \"EndTime\": 1712784181.891324, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6071.788549423218, \"count\": 1, \"min\": 6071.788549423218, \"max\": 6071.788549423218}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:01 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.73331726044093 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:01 INFO 139627334805312] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.163611347025091\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:01 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:02 INFO 139627334805312] Epoch[65] Batch[0] avg_epoch_loss=2.158407\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=2.15840744972229\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:05 INFO 139627334805312] Epoch[65] Batch[5] avg_epoch_loss=2.127644\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.1276440620422363\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:05 INFO 139627334805312] Epoch[65] Batch [5]#011Speed: 109.35 samples/sec#011loss=2.127644\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] Epoch[65] Batch[10] avg_epoch_loss=2.131912\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=2.137033987045288\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] Epoch[65] Batch [10]#011Speed: 119.23 samples/sec#011loss=2.137034\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784181.891386, \"EndTime\": 1712784188.2641597, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6372.456073760986, \"count\": 1, \"min\": 6372.456073760986, \"max\": 6372.456073760986}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=101.05845716443504 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.131912209770896\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] Epoch[66] Batch[0] avg_epoch_loss=2.123185\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=2.1231846809387207\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:11 INFO 139627334805312] Epoch[66] Batch[5] avg_epoch_loss=2.161456\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=2.161456346511841\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:11 INFO 139627334805312] Epoch[66] Batch [5]#011Speed: 119.56 samples/sec#011loss=2.161456\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:14 INFO 139627334805312] Epoch[66] Batch[10] avg_epoch_loss=2.155526\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=2.1484102249145507\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:14 INFO 139627334805312] Epoch[66] Batch [10]#011Speed: 115.31 samples/sec#011loss=2.148410\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:14 INFO 139627334805312] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784188.2642238, \"EndTime\": 1712784194.441423, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6176.898717880249, \"count\": 1, \"min\": 6176.898717880249, \"max\": 6176.898717880249}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:14 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.17182334767962 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:14 INFO 139627334805312] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=66, train loss <loss>=2.1555262912403452\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:14 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:15 INFO 139627334805312] Epoch[67] Batch[0] avg_epoch_loss=2.119807\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=2.119807243347168\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:17 INFO 139627334805312] Epoch[67] Batch[5] avg_epoch_loss=2.126947\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=2.126947363217672\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:17 INFO 139627334805312] Epoch[67] Batch [5]#011Speed: 115.04 samples/sec#011loss=2.126947\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] Epoch[67] Batch[10] avg_epoch_loss=2.096055\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=2.0589842319488527\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] Epoch[67] Batch [10]#011Speed: 120.26 samples/sec#011loss=2.058984\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784194.4414895, \"EndTime\": 1712784200.6104965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6168.701887130737, \"count\": 1, \"min\": 6168.701887130737, \"max\": 6168.701887130737}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=103.9100128202953 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=67, train loss <loss>=2.096055030822754\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:20 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_bd7df4ef-0f67-4a56-93c4-bcacca8f5573-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784200.6105628, \"EndTime\": 1712784200.6887617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.86965370178223, \"count\": 1, \"min\": 77.86965370178223, \"max\": 77.86965370178223}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:21 INFO 139627334805312] Epoch[68] Batch[0] avg_epoch_loss=2.151195\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=2.1511948108673096\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:24 INFO 139627334805312] Epoch[68] Batch[5] avg_epoch_loss=2.130886\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=2.130885601043701\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:24 INFO 139627334805312] Epoch[68] Batch [5]#011Speed: 120.53 samples/sec#011loss=2.130886\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:26 INFO 139627334805312] Epoch[68] Batch[10] avg_epoch_loss=2.123346\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=2.114298629760742\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:26 INFO 139627334805312] Epoch[68] Batch [10]#011Speed: 118.62 samples/sec#011loss=2.114299\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:26 INFO 139627334805312] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784200.6888258, \"EndTime\": 1712784206.7524424, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6063.560009002686, \"count\": 1, \"min\": 6063.560009002686, \"max\": 6063.560009002686}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:26 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.98913105192425 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:26 INFO 139627334805312] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=68, train loss <loss>=2.123346068642356\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:26 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:27 INFO 139627334805312] Epoch[69] Batch[0] avg_epoch_loss=2.091486\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.0914864540100098\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:23:30 INFO 139627334805312] Epoch[69] Batch[5] avg_epoch_loss=2.132741\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.1327414512634277\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:30 INFO 139627334805312] Epoch[69] Batch [5]#011Speed: 120.58 samples/sec#011loss=2.132741\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:32 INFO 139627334805312] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784206.7525072, \"EndTime\": 1712784212.2495306, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5496.672630310059, \"count\": 1, \"min\": 5496.672630310059, \"max\": 5496.672630310059}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:32 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.24756488352283 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:32 INFO 139627334805312] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.1182591915130615\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:32 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:32 INFO 139627334805312] Epoch[70] Batch[0] avg_epoch_loss=2.086318\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=2.0863184928894043\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:35 INFO 139627334805312] Epoch[70] Batch[5] avg_epoch_loss=2.088192\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=2.0881923039754233\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:35 INFO 139627334805312] Epoch[70] Batch [5]#011Speed: 120.60 samples/sec#011loss=2.088192\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:37 INFO 139627334805312] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784212.2496028, \"EndTime\": 1712784217.7374701, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5487.524509429932, \"count\": 1, \"min\": 5487.524509429932, \"max\": 5487.524509429932}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:37 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.43474512375276 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:37 INFO 139627334805312] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=70, train loss <loss>=2.0982130765914917\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:37 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:38 INFO 139627334805312] Epoch[71] Batch[0] avg_epoch_loss=2.067611\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=2.0676112174987793\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:41 INFO 139627334805312] Epoch[71] Batch[5] avg_epoch_loss=2.108529\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=2.1085288524627686\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:41 INFO 139627334805312] Epoch[71] Batch [5]#011Speed: 119.36 samples/sec#011loss=2.108529\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:43 INFO 139627334805312] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784217.7375412, \"EndTime\": 1712784223.2579648, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5520.075082778931, \"count\": 1, \"min\": 5520.075082778931, \"max\": 5520.075082778931}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:43 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.77175647278003 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:43 INFO 139627334805312] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=71, train loss <loss>=2.1055379152297973\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:43 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:43 INFO 139627334805312] Epoch[72] Batch[0] avg_epoch_loss=2.152555\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=2.152554750442505\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:46 INFO 139627334805312] Epoch[72] Batch[5] avg_epoch_loss=2.112319\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=2.112319032351176\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:46 INFO 139627334805312] Epoch[72] Batch [5]#011Speed: 121.49 samples/sec#011loss=2.112319\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] Epoch[72] Batch[10] avg_epoch_loss=2.102159\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=2.0899675130844115\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] Epoch[72] Batch [10]#011Speed: 120.51 samples/sec#011loss=2.089968\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784223.258036, \"EndTime\": 1712784229.2525675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5994.105815887451, \"count\": 1, \"min\": 5994.105815887451, \"max\": 5994.105815887451}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.10547728723051 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=72, train loss <loss>=2.102159250866283\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] Epoch[73] Batch[0] avg_epoch_loss=2.073388\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.0733883380889893\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:52 INFO 139627334805312] Epoch[73] Batch[5] avg_epoch_loss=2.124291\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=2.1242913802464805\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:52 INFO 139627334805312] Epoch[73] Batch [5]#011Speed: 120.98 samples/sec#011loss=2.124291\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] Epoch[73] Batch[10] avg_epoch_loss=2.096409\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=2.062950944900513\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] Epoch[73] Batch [10]#011Speed: 120.58 samples/sec#011loss=2.062951\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784229.252682, \"EndTime\": 1712784235.262737, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6009.679079055786, \"count\": 1, \"min\": 6009.679079055786, \"max\": 6009.679079055786}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.48992419591808 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=73, train loss <loss>=2.0964093641801314\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] Epoch[74] Batch[0] avg_epoch_loss=2.149257\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=2.149257183074951\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:58 INFO 139627334805312] Epoch[74] Batch[5] avg_epoch_loss=2.105528\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=2.105527877807617\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:23:58 INFO 139627334805312] Epoch[74] Batch [5]#011Speed: 120.21 samples/sec#011loss=2.105528\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] Epoch[74] Batch[10] avg_epoch_loss=2.083623\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=2.05733745098114\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] Epoch[74] Batch [10]#011Speed: 118.96 samples/sec#011loss=2.057337\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784235.2628024, \"EndTime\": 1712784241.3306673, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6067.568063735962, \"count\": 1, \"min\": 6067.568063735962, \"max\": 6067.568063735962}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.13637021263253 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=74, train loss <loss>=2.0836231383410366\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:01 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_9e3cc41b-29b7-484c-8ea5-2085bcf2b150-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784241.3307338, \"EndTime\": 1712784241.4097369, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.60207557678223, \"count\": 1, \"min\": 78.60207557678223, \"max\": 78.60207557678223}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:02 INFO 139627334805312] Epoch[75] Batch[0] avg_epoch_loss=2.115076\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=2.1150763034820557\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:05 INFO 139627334805312] Epoch[75] Batch[5] avg_epoch_loss=2.093802\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=2.0938022136688232\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:05 INFO 139627334805312] Epoch[75] Batch [5]#011Speed: 110.66 samples/sec#011loss=2.093802\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:07 INFO 139627334805312] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784241.4098048, \"EndTime\": 1712784247.2079084, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5798.045635223389, \"count\": 1, \"min\": 5798.045635223389, \"max\": 5798.045635223389}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:07 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.58567632983534 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:07 INFO 139627334805312] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=75, train loss <loss>=2.096128535270691\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:07 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:07 INFO 139627334805312] Epoch[76] Batch[0] avg_epoch_loss=2.095882\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=2.0958821773529053\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:24:10 INFO 139627334805312] Epoch[76] Batch[5] avg_epoch_loss=2.078033\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=2.0780327320098877\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:10 INFO 139627334805312] Epoch[76] Batch [5]#011Speed: 120.14 samples/sec#011loss=2.078033\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] Epoch[76] Batch[10] avg_epoch_loss=2.100316\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=2.127055025100708\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] Epoch[76] Batch [10]#011Speed: 119.25 samples/sec#011loss=2.127055\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784247.207982, \"EndTime\": 1712784253.2667184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6058.316707611084, \"count\": 1, \"min\": 6058.316707611084, \"max\": 6058.316707611084}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.92008409954839 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=76, train loss <loss>=2.1003155925057153\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] Epoch[77] Batch[0] avg_epoch_loss=2.037377\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.037376642227173\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:16 INFO 139627334805312] Epoch[77] Batch[5] avg_epoch_loss=2.064792\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.0647917985916138\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:16 INFO 139627334805312] Epoch[77] Batch [5]#011Speed: 120.31 samples/sec#011loss=2.064792\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] Epoch[77] Batch[10] avg_epoch_loss=2.039375\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=2.008875608444214\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] Epoch[77] Batch [10]#011Speed: 119.88 samples/sec#011loss=2.008876\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784253.2667856, \"EndTime\": 1712784259.303826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6036.734104156494, \"count\": 1, \"min\": 6036.734104156494, \"max\": 6036.734104156494}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.51289823299813 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=77, train loss <loss>=2.0393753485246138\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:19 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_1ffdac9e-dabb-4054-bcc3-6b1dfae8451a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784259.30389, \"EndTime\": 1712784259.3821397, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.9263973236084, \"count\": 1, \"min\": 77.9263973236084, \"max\": 77.9263973236084}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:20 INFO 139627334805312] Epoch[78] Batch[0] avg_epoch_loss=2.124110\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=2.124110460281372\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:22 INFO 139627334805312] Epoch[78] Batch[5] avg_epoch_loss=2.088287\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:22 INFO 139627334805312] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=2.0882869958877563\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:22 INFO 139627334805312] Epoch[78] Batch [5]#011Speed: 119.05 samples/sec#011loss=2.088287\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:25 INFO 139627334805312] Epoch[78] Batch[10] avg_epoch_loss=2.064282\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=2.035475492477417\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:25 INFO 139627334805312] Epoch[78] Batch [10]#011Speed: 119.67 samples/sec#011loss=2.035475\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:25 INFO 139627334805312] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784259.3822057, \"EndTime\": 1712784265.4421697, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6059.908151626587, \"count\": 1, \"min\": 6059.908151626587, \"max\": 6059.908151626587}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:25 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.5412197663892 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:25 INFO 139627334805312] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=78, train loss <loss>=2.064281767064875\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:25 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:26 INFO 139627334805312] Epoch[79] Batch[0] avg_epoch_loss=2.098519\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.0985193252563477\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:28 INFO 139627334805312] Epoch[79] Batch[5] avg_epoch_loss=2.112884\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:28 INFO 139627334805312] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=2.1128838459650674\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:28 INFO 139627334805312] Epoch[79] Batch [5]#011Speed: 120.57 samples/sec#011loss=2.112884\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:30 INFO 139627334805312] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784265.4422338, \"EndTime\": 1712784270.9221544, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5479.575634002686, \"count\": 1, \"min\": 5479.575634002686, \"max\": 5479.575634002686}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:30 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.70024095181951 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:30 INFO 139627334805312] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=79, train loss <loss>=2.0836105227470396\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:30 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:31 INFO 139627334805312] Epoch[80] Batch[0] avg_epoch_loss=2.113376\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=2.1133759021759033\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:34 INFO 139627334805312] Epoch[80] Batch[5] avg_epoch_loss=2.085456\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=2.0854564110438027\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:34 INFO 139627334805312] Epoch[80] Batch [5]#011Speed: 118.69 samples/sec#011loss=2.085456\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] Epoch[80] Batch[10] avg_epoch_loss=2.062386\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=2.034700798988342\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] Epoch[80] Batch [10]#011Speed: 117.90 samples/sec#011loss=2.034701\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784270.9222252, \"EndTime\": 1712784277.0332596, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6110.531330108643, \"count\": 1, \"min\": 6110.531330108643, \"max\": 6110.531330108643}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.04478610893202 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=80, train loss <loss>=2.062385678291321\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] Epoch[81] Batch[0] avg_epoch_loss=2.038260\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=2.0382604598999023\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:24:40 INFO 139627334805312] Epoch[81] Batch[5] avg_epoch_loss=2.076058\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=2.0760576725006104\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:40 INFO 139627334805312] Epoch[81] Batch [5]#011Speed: 119.11 samples/sec#011loss=2.076058\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] Epoch[81] Batch[10] avg_epoch_loss=2.031154\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=1.977269458770752\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] Epoch[81] Batch [10]#011Speed: 119.64 samples/sec#011loss=1.977269\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784277.033323, \"EndTime\": 1712784283.1107492, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6077.128171920776, \"count\": 1, \"min\": 6077.128171920776, \"max\": 6077.128171920776}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.6403547139649 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=81, train loss <loss>=2.0311539389870386\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_cf45fcf1-5b4f-4954-856f-3f29e109a116-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784283.110815, \"EndTime\": 1712784283.1886048, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.46696472167969, \"count\": 1, \"min\": 77.46696472167969, \"max\": 77.46696472167969}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] Epoch[82] Batch[0] avg_epoch_loss=2.163937\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=2.1639366149902344\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:46 INFO 139627334805312] Epoch[82] Batch[5] avg_epoch_loss=2.115223\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=2.115223209063212\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:46 INFO 139627334805312] Epoch[82] Batch [5]#011Speed: 120.70 samples/sec#011loss=2.115223\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] Epoch[82] Batch[10] avg_epoch_loss=2.094375\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=2.069358062744141\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] Epoch[82] Batch [10]#011Speed: 120.38 samples/sec#011loss=2.069358\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784283.1886718, \"EndTime\": 1712784289.2092354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6020.5066204071045, \"count\": 1, \"min\": 6020.5066204071045, \"max\": 6020.5066204071045}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.62699836743182 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=82, train loss <loss>=2.094375415281816\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] Epoch[83] Batch[0] avg_epoch_loss=2.008229\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=2.0082285404205322\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:52 INFO 139627334805312] Epoch[83] Batch[5] avg_epoch_loss=2.053879\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=2.0538790225982666\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:52 INFO 139627334805312] Epoch[83] Batch [5]#011Speed: 120.33 samples/sec#011loss=2.053879\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] Epoch[83] Batch[10] avg_epoch_loss=2.043894\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=2.031911325454712\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] Epoch[83] Batch [10]#011Speed: 120.24 samples/sec#011loss=2.031911\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784289.2093017, \"EndTime\": 1712784295.2457867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6036.171197891235, \"count\": 1, \"min\": 6036.171197891235, \"max\": 6036.171197891235}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.16142614621404 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=83, train loss <loss>=2.0438937057148325\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] Epoch[84] Batch[0] avg_epoch_loss=2.053417\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=2.0534169673919678\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:58 INFO 139627334805312] Epoch[84] Batch[5] avg_epoch_loss=2.058323\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=2.05832310517629\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:24:58 INFO 139627334805312] Epoch[84] Batch [5]#011Speed: 121.13 samples/sec#011loss=2.058323\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:00 INFO 139627334805312] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784295.2458525, \"EndTime\": 1712784300.7486715, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5502.530097961426, \"count\": 1, \"min\": 5502.530097961426, \"max\": 5502.530097961426}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:00 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.5819278286122 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:00 INFO 139627334805312] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=84, train loss <loss>=2.049831974506378\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:00 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:01 INFO 139627334805312] Epoch[85] Batch[0] avg_epoch_loss=1.954360\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=1.954359769821167\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:04 INFO 139627334805312] Epoch[85] Batch[5] avg_epoch_loss=2.062554\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=2.062554041544596\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:04 INFO 139627334805312] Epoch[85] Batch [5]#011Speed: 109.52 samples/sec#011loss=2.062554\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:06 INFO 139627334805312] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784300.7487447, \"EndTime\": 1712784306.5800064, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5830.861568450928, \"count\": 1, \"min\": 5830.861568450928, \"max\": 5830.861568450928}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:06 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=104.7853349909463 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:06 INFO 139627334805312] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:06 INFO 139627334805312] #quality_metric: host=algo-1, epoch=85, train loss <loss>=2.0800760269165037\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:06 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:07 INFO 139627334805312] Epoch[86] Batch[0] avg_epoch_loss=2.042781\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=2.042781352996826\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:25:09 INFO 139627334805312] Epoch[86] Batch[5] avg_epoch_loss=2.047454\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=2.0474543372790017\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:09 INFO 139627334805312] Epoch[86] Batch [5]#011Speed: 120.69 samples/sec#011loss=2.047454\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] Epoch[86] Batch[10] avg_epoch_loss=2.028859\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=2.006544017791748\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] Epoch[86] Batch [10]#011Speed: 120.02 samples/sec#011loss=2.006544\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784306.5800793, \"EndTime\": 1712784312.6284404, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6048.01607131958, \"count\": 1, \"min\": 6048.01607131958, \"max\": 6048.01607131958}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.64485618457137 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=86, train loss <loss>=2.0288587375120684\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:12 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_71c496a0-92ac-403c-ae77-eb37a007e7d1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784312.6285067, \"EndTime\": 1712784312.7064636, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.63075828552246, \"count\": 1, \"min\": 77.63075828552246, \"max\": 77.63075828552246}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:13 INFO 139627334805312] Epoch[87] Batch[0] avg_epoch_loss=2.037565\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=2.037564516067505\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:16 INFO 139627334805312] Epoch[87] Batch[5] avg_epoch_loss=2.059257\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=2.059257467587789\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:16 INFO 139627334805312] Epoch[87] Batch [5]#011Speed: 120.62 samples/sec#011loss=2.059257\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:18 INFO 139627334805312] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784312.7065284, \"EndTime\": 1712784318.195682, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5489.095687866211, \"count\": 1, \"min\": 5489.095687866211, \"max\": 5489.095687866211}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:18 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.9450973749293 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:18 INFO 139627334805312] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=87, train loss <loss>=2.039630424976349\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:18 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:18 INFO 139627334805312] Epoch[88] Batch[0] avg_epoch_loss=2.088053\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=2.088052749633789\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:21 INFO 139627334805312] Epoch[88] Batch[5] avg_epoch_loss=2.069324\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=2.069324175516764\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:21 INFO 139627334805312] Epoch[88] Batch [5]#011Speed: 121.25 samples/sec#011loss=2.069324\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] Epoch[88] Batch[10] avg_epoch_loss=2.131278\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=2.2056232929229735\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] Epoch[88] Batch [10]#011Speed: 119.47 samples/sec#011loss=2.205623\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784318.1957543, \"EndTime\": 1712784324.2227652, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6026.586294174194, \"count\": 1, \"min\": 6026.586294174194, \"max\": 6026.586294174194}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.52625412507177 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=88, train loss <loss>=2.131278319792314\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] Epoch[89] Batch[0] avg_epoch_loss=2.025642\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=2.0256423950195312\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:27 INFO 139627334805312] Epoch[89] Batch[5] avg_epoch_loss=2.025216\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=2.0252158641815186\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:27 INFO 139627334805312] Epoch[89] Batch [5]#011Speed: 119.74 samples/sec#011loss=2.025216\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] Epoch[89] Batch[10] avg_epoch_loss=2.029267\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=2.0341282367706297\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] Epoch[89] Batch [10]#011Speed: 120.17 samples/sec#011loss=2.034128\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784324.2228317, \"EndTime\": 1712784330.2638958, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6040.76886177063, \"count\": 1, \"min\": 6040.76886177063, \"max\": 6040.76886177063}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.43499703124401 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=89, train loss <loss>=2.0292669426311147\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] Epoch[90] Batch[0] avg_epoch_loss=2.064043\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=2.0640430450439453\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:33 INFO 139627334805312] Epoch[90] Batch[5] avg_epoch_loss=2.077852\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=2.0778515736262\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:33 INFO 139627334805312] Epoch[90] Batch [5]#011Speed: 119.66 samples/sec#011loss=2.077852\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] Epoch[90] Batch[10] avg_epoch_loss=2.026918\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=1.965798282623291\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] Epoch[90] Batch [10]#011Speed: 118.97 samples/sec#011loss=1.965798\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784330.2639606, \"EndTime\": 1712784336.336618, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6072.364807128906, \"count\": 1, \"min\": 6072.364807128906, \"max\": 6072.364807128906}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.99265302494302 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=90, train loss <loss>=2.0269182595339688\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:36 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_50601c53-f145-4eca-9bf9-6475ca72b2ff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784336.3366911, \"EndTime\": 1712784336.4142687, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.1489143371582, \"count\": 1, \"min\": 77.1489143371582, \"max\": 77.1489143371582}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:37 INFO 139627334805312] Epoch[91] Batch[0] avg_epoch_loss=1.986998\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=1.9869980812072754\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:25:39 INFO 139627334805312] Epoch[91] Batch[5] avg_epoch_loss=2.032761\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=2.0327606995900473\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:39 INFO 139627334805312] Epoch[91] Batch [5]#011Speed: 118.48 samples/sec#011loss=2.032761\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:41 INFO 139627334805312] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784336.4143429, \"EndTime\": 1712784341.9846444, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5570.213079452515, \"count\": 1, \"min\": 5570.213079452515, \"max\": 5570.213079452515}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:41 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.58579140051641 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:41 INFO 139627334805312] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=91, train loss <loss>=2.014221739768982\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:41 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:42 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_ef4b989d-7006-48ec-9539-418479df8577-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784341.9847298, \"EndTime\": 1712784342.0652761, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 80.07478713989258, \"count\": 1, \"min\": 80.07478713989258, \"max\": 80.07478713989258}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:42 INFO 139627334805312] Epoch[92] Batch[0] avg_epoch_loss=2.015965\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=2.015964984893799\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:45 INFO 139627334805312] Epoch[92] Batch[5] avg_epoch_loss=2.001814\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=2.001813530921936\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:45 INFO 139627334805312] Epoch[92] Batch [5]#011Speed: 118.84 samples/sec#011loss=2.001814\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:47 INFO 139627334805312] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784342.0653448, \"EndTime\": 1712784347.6377287, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5572.322607040405, \"count\": 1, \"min\": 5572.322607040405, \"max\": 5572.322607040405}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:47 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.006162053598 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:47 INFO 139627334805312] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=92, train loss <loss>=2.00450519323349\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:47 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:47 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_f87eb454-42cb-44db-abe3-c4a4ac69cbbe-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784347.6377904, \"EndTime\": 1712784347.7156303, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.35204696655273, \"count\": 1, \"min\": 77.35204696655273, \"max\": 77.35204696655273}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:48 INFO 139627334805312] Epoch[93] Batch[0] avg_epoch_loss=2.033895\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=2.033895492553711\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:51 INFO 139627334805312] Epoch[93] Batch[5] avg_epoch_loss=2.020343\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=2.020343005657196\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:51 INFO 139627334805312] Epoch[93] Batch [5]#011Speed: 120.54 samples/sec#011loss=2.020343\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:53 INFO 139627334805312] Epoch[93] Batch[10] avg_epoch_loss=2.040726\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=2.065186595916748\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:53 INFO 139627334805312] Epoch[93] Batch [10]#011Speed: 119.98 samples/sec#011loss=2.065187\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:53 INFO 139627334805312] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784347.715699, \"EndTime\": 1712784353.74773, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6031.978607177734, \"count\": 1, \"min\": 6031.978607177734, \"max\": 6031.978607177734}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:53 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.91248861379027 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:53 INFO 139627334805312] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=93, train loss <loss>=2.0407264557751743\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:53 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:54 INFO 139627334805312] Epoch[94] Batch[0] avg_epoch_loss=2.076443\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=2.0764429569244385\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:57 INFO 139627334805312] Epoch[94] Batch[5] avg_epoch_loss=2.056701\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=2.0567013025283813\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:25:57 INFO 139627334805312] Epoch[94] Batch [5]#011Speed: 115.96 samples/sec#011loss=2.056701\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] Epoch[94] Batch[10] avg_epoch_loss=2.013584\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=1.9618443250656128\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] Epoch[94] Batch [10]#011Speed: 114.43 samples/sec#011loss=1.961844\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784353.7477934, \"EndTime\": 1712784360.0110972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6263.012647628784, \"count\": 1, \"min\": 6263.012647628784, \"max\": 6263.012647628784}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=102.5049399873975 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=94, train loss <loss>=2.0135844945907593\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] Epoch[95] Batch[0] avg_epoch_loss=2.203080\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=2.20307993888855\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:03 INFO 139627334805312] Epoch[95] Batch[5] avg_epoch_loss=2.143392\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=2.1433915297190347\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:03 INFO 139627334805312] Epoch[95] Batch [5]#011Speed: 101.72 samples/sec#011loss=2.143392\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:06 INFO 139627334805312] Epoch[95] Batch[10] avg_epoch_loss=2.150136\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:06 INFO 139627334805312] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=2.158229422569275\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:06 INFO 139627334805312] Epoch[95] Batch [10]#011Speed: 113.78 samples/sec#011loss=2.158229\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:06 INFO 139627334805312] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784360.0111668, \"EndTime\": 1712784366.70563, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6694.07844543457, \"count\": 1, \"min\": 6694.07844543457, \"max\": 6694.07844543457}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:06 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=96.20296674620505 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:06 INFO 139627334805312] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:06 INFO 139627334805312] #quality_metric: host=algo-1, epoch=95, train loss <loss>=2.150136026469144\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:06 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:07 INFO 139627334805312] Epoch[96] Batch[0] avg_epoch_loss=2.063165\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=2.0631654262542725\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:26:10 INFO 139627334805312] Epoch[96] Batch[5] avg_epoch_loss=2.131168\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=2.1311678886413574\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:10 INFO 139627334805312] Epoch[96] Batch [5]#011Speed: 113.96 samples/sec#011loss=2.131168\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:12 INFO 139627334805312] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784366.7056997, \"EndTime\": 1712784372.5180721, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5812.025785446167, \"count\": 1, \"min\": 5812.025785446167, \"max\": 5812.025785446167}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:12 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.18953588171955 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:12 INFO 139627334805312] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=96, train loss <loss>=2.1219510793685914\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:12 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:13 INFO 139627334805312] Epoch[97] Batch[0] avg_epoch_loss=2.083530\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=2.0835304260253906\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:16 INFO 139627334805312] Epoch[97] Batch[5] avg_epoch_loss=2.085683\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=2.0856828689575195\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:16 INFO 139627334805312] Epoch[97] Batch [5]#011Speed: 113.02 samples/sec#011loss=2.085683\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:18 INFO 139627334805312] Epoch[97] Batch[10] avg_epoch_loss=2.079101\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=2.0712030649185182\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:18 INFO 139627334805312] Epoch[97] Batch [10]#011Speed: 118.36 samples/sec#011loss=2.071203\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:18 INFO 139627334805312] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784372.5181463, \"EndTime\": 1712784378.787373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6268.837928771973, \"count\": 1, \"min\": 6268.837928771973, \"max\": 6268.837928771973}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:18 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.51425899535415 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:18 INFO 139627334805312] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=97, train loss <loss>=2.0791011398488823\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:18 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:19 INFO 139627334805312] Epoch[98] Batch[0] avg_epoch_loss=2.173263\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=2.1732630729675293\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:22 INFO 139627334805312] Epoch[98] Batch[5] avg_epoch_loss=2.054045\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:22 INFO 139627334805312] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=2.0540451407432556\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:22 INFO 139627334805312] Epoch[98] Batch [5]#011Speed: 120.08 samples/sec#011loss=2.054045\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:24 INFO 139627334805312] Epoch[98] Batch[10] avg_epoch_loss=2.076101\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=2.1025671482086183\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:24 INFO 139627334805312] Epoch[98] Batch [10]#011Speed: 119.74 samples/sec#011loss=2.102567\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:24 INFO 139627334805312] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784378.7874398, \"EndTime\": 1712784384.8277035, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6039.95680809021, \"count\": 1, \"min\": 6039.95680809021, \"max\": 6039.95680809021}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:24 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.91960276320107 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:24 INFO 139627334805312] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=98, train loss <loss>=2.0761005986820567\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:24 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:25 INFO 139627334805312] Epoch[99] Batch[0] avg_epoch_loss=2.003599\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=2.003599166870117\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:28 INFO 139627334805312] Epoch[99] Batch[5] avg_epoch_loss=2.017087\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:28 INFO 139627334805312] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=2.0170868237813315\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:28 INFO 139627334805312] Epoch[99] Batch [5]#011Speed: 120.39 samples/sec#011loss=2.017087\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:30 INFO 139627334805312] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784384.8277678, \"EndTime\": 1712784390.332675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5504.615545272827, \"count\": 1, \"min\": 5504.615545272827, \"max\": 5504.615545272827}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:30 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=116.08224940434526 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:30 INFO 139627334805312] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=99, train loss <loss>=2.017910099029541\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:30 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:31 INFO 139627334805312] Epoch[100] Batch[0] avg_epoch_loss=2.041174\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=2.0411736965179443\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:33 INFO 139627334805312] Epoch[100] Batch[5] avg_epoch_loss=2.007459\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=2.0074587861696878\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:33 INFO 139627334805312] Epoch[100] Batch [5]#011Speed: 119.11 samples/sec#011loss=2.007459\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:36 INFO 139627334805312] Epoch[100] Batch[10] avg_epoch_loss=2.015104\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=2.024277901649475\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:36 INFO 139627334805312] Epoch[100] Batch [10]#011Speed: 117.87 samples/sec#011loss=2.024278\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:36 INFO 139627334805312] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784390.3327456, \"EndTime\": 1712784396.4466271, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6113.527536392212, \"count\": 1, \"min\": 6113.527536392212, \"max\": 6113.527536392212}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:36 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.08200098594519 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:36 INFO 139627334805312] #progress_metric: host=algo-1, completed 50.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=100, train loss <loss>=2.0151038386605005\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:36 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:37 INFO 139627334805312] Epoch[101] Batch[0] avg_epoch_loss=2.001937\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=2.001936912536621\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:39 INFO 139627334805312] Epoch[101] Batch[5] avg_epoch_loss=1.986133\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=1.986133337020874\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:39 INFO 139627334805312] Epoch[101] Batch [5]#011Speed: 119.13 samples/sec#011loss=1.986133\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:42 INFO 139627334805312] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784396.446693, \"EndTime\": 1712784402.0093682, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5562.372922897339, \"count\": 1, \"min\": 5562.372922897339, \"max\": 5562.372922897339}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:42 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.25887843252865 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:42 INFO 139627334805312] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=101, train loss <loss>=1.9938629031181336\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:42 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:42 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_0f46e1f6-156e-4522-8e70-1a528c050a8b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784402.00944, \"EndTime\": 1712784402.091998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 82.18860626220703, \"count\": 1, \"min\": 82.18860626220703, \"max\": 82.18860626220703}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:42 INFO 139627334805312] Epoch[102] Batch[0] avg_epoch_loss=2.044190\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=2.0441904067993164\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:45 INFO 139627334805312] Epoch[102] Batch[5] avg_epoch_loss=2.013815\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=2.0138151049613953\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:45 INFO 139627334805312] Epoch[102] Batch [5]#011Speed: 119.83 samples/sec#011loss=2.013815\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:47 INFO 139627334805312] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784402.092072, \"EndTime\": 1712784407.636455, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5544.323205947876, \"count\": 1, \"min\": 5544.323205947876, \"max\": 5544.323205947876}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:47 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.18473301022016 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:47 INFO 139627334805312] #progress_metric: host=algo-1, completed 51.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=102, train loss <loss>=2.0060511589050294\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:47 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:48 INFO 139627334805312] Epoch[103] Batch[0] avg_epoch_loss=2.044342\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=2.044342041015625\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:26:50 INFO 139627334805312] Epoch[103] Batch[5] avg_epoch_loss=2.016136\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=2.0161363085110984\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:50 INFO 139627334805312] Epoch[103] Batch [5]#011Speed: 120.61 samples/sec#011loss=2.016136\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:53 INFO 139627334805312] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784407.6365259, \"EndTime\": 1712784413.1150484, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5478.144884109497, \"count\": 1, \"min\": 5478.144884109497, \"max\": 5478.144884109497}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:53 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.72252154733921 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:53 INFO 139627334805312] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=103, train loss <loss>=2.012505328655243\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:53 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:53 INFO 139627334805312] Epoch[104] Batch[0] avg_epoch_loss=1.979598\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=1.979597806930542\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:56 INFO 139627334805312] Epoch[104] Batch[5] avg_epoch_loss=1.982315\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=1.9823146263758342\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:56 INFO 139627334805312] Epoch[104] Batch [5]#011Speed: 120.36 samples/sec#011loss=1.982315\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:58 INFO 139627334805312] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784413.115118, \"EndTime\": 1712784418.5937939, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5478.189706802368, \"count\": 1, \"min\": 5478.189706802368, \"max\": 5478.189706802368}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:58 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.35647921155997 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:58 INFO 139627334805312] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=104, train loss <loss>=2.0069299221038817\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:58 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:59 INFO 139627334805312] Epoch[105] Batch[0] avg_epoch_loss=2.008727\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:26:59 INFO 139627334805312] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=2.0087265968322754\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:02 INFO 139627334805312] Epoch[105] Batch[5] avg_epoch_loss=2.014277\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=2.0142770210901895\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:02 INFO 139627334805312] Epoch[105] Batch [5]#011Speed: 117.15 samples/sec#011loss=2.014277\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:04 INFO 139627334805312] Epoch[105] Batch[10] avg_epoch_loss=2.049627\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=2.0920460700988768\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:04 INFO 139627334805312] Epoch[105] Batch [10]#011Speed: 111.11 samples/sec#011loss=2.092046\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:04 INFO 139627334805312] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784418.5938644, \"EndTime\": 1712784424.909597, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6315.343618392944, \"count\": 1, \"min\": 6315.343618392944, \"max\": 6315.343618392944}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:04 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=101.6556028923985 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:04 INFO 139627334805312] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=105, train loss <loss>=2.049626588821411\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:04 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:05 INFO 139627334805312] Epoch[106] Batch[0] avg_epoch_loss=1.898817\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=1.8988174200057983\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:08 INFO 139627334805312] Epoch[106] Batch[5] avg_epoch_loss=1.963689\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=1.963689108689626\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:08 INFO 139627334805312] Epoch[106] Batch [5]#011Speed: 119.97 samples/sec#011loss=1.963689\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:10 INFO 139627334805312] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784424.9096637, \"EndTime\": 1712784430.3980248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5488.018274307251, \"count\": 1, \"min\": 5488.018274307251, \"max\": 5488.018274307251}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:10 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.97121056385937 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:10 INFO 139627334805312] #progress_metric: host=algo-1, completed 53.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=106, train loss <loss>=1.979610228538513\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:10 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:10 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_d772d608-f50f-4fca-a851-100d0086e0a6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784430.3980968, \"EndTime\": 1712784430.4762192, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.55684852600098, \"count\": 1, \"min\": 77.55684852600098, \"max\": 77.55684852600098}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:11 INFO 139627334805312] Epoch[107] Batch[0] avg_epoch_loss=1.996189\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=1.9961885213851929\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:13 INFO 139627334805312] Epoch[107] Batch[5] avg_epoch_loss=1.983882\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=1.9838817715644836\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:13 INFO 139627334805312] Epoch[107] Batch [5]#011Speed: 120.35 samples/sec#011loss=1.983882\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:15 INFO 139627334805312] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784430.4762857, \"EndTime\": 1712784435.9698126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5493.467569351196, \"count\": 1, \"min\": 5493.467569351196, \"max\": 5493.467569351196}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:15 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.86153950579875 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:15 INFO 139627334805312] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=107, train loss <loss>=1.9812824606895447\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:15 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:16 INFO 139627334805312] Epoch[108] Batch[0] avg_epoch_loss=1.970304\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=1.9703037738800049\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:19 INFO 139627334805312] Epoch[108] Batch[5] avg_epoch_loss=1.994629\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=1.9946288466453552\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:19 INFO 139627334805312] Epoch[108] Batch [5]#011Speed: 120.93 samples/sec#011loss=1.994629\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:21 INFO 139627334805312] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784435.9698834, \"EndTime\": 1712784441.4687684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5498.499155044556, \"count\": 1, \"min\": 5498.499155044556, \"max\": 5498.499155044556}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:21 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.75590078661256 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:21 INFO 139627334805312] #progress_metric: host=algo-1, completed 54.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=108, train loss <loss>=1.976935613155365\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:21 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:21 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_0177d299-9fdd-4f1a-812c-841c44b2c669-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784441.468841, \"EndTime\": 1712784441.5467262, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.48866081237793, \"count\": 1, \"min\": 77.48866081237793, \"max\": 77.48866081237793}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:22 INFO 139627334805312] Epoch[109] Batch[0] avg_epoch_loss=2.049796\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:22 INFO 139627334805312] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=2.0497961044311523\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:24 INFO 139627334805312] Epoch[109] Batch[5] avg_epoch_loss=2.012729\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=2.0127290884653726\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:24 INFO 139627334805312] Epoch[109] Batch [5]#011Speed: 119.39 samples/sec#011loss=2.012729\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:27 INFO 139627334805312] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784441.5467908, \"EndTime\": 1712784447.0687592, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5521.909952163696, \"count\": 1, \"min\": 5521.909952163696, \"max\": 5521.909952163696}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:27 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.54554556836531 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:27 INFO 139627334805312] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=109, train loss <loss>=1.9969030618667603\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:27 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:27 INFO 139627334805312] Epoch[110] Batch[0] avg_epoch_loss=2.014277\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=2.0142767429351807\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:27:30 INFO 139627334805312] Epoch[110] Batch[5] avg_epoch_loss=2.004340\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=2.0043402115503945\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:30 INFO 139627334805312] Epoch[110] Batch [5]#011Speed: 118.90 samples/sec#011loss=2.004340\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:32 INFO 139627334805312] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784447.0688293, \"EndTime\": 1712784452.6333916, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5564.184188842773, \"count\": 1, \"min\": 5564.184188842773, \"max\": 5564.184188842773}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:32 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.2682497429656 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:32 INFO 139627334805312] #progress_metric: host=algo-1, completed 55.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=110, train loss <loss>=2.018871378898621\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:32 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:33 INFO 139627334805312] Epoch[111] Batch[0] avg_epoch_loss=1.910500\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=1.910500168800354\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:36 INFO 139627334805312] Epoch[111] Batch[5] avg_epoch_loss=1.967387\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=1.9673871596654255\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:36 INFO 139627334805312] Epoch[111] Batch [5]#011Speed: 117.97 samples/sec#011loss=1.967387\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] Epoch[111] Batch[10] avg_epoch_loss=1.955152\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=1.940470552444458\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] Epoch[111] Batch [10]#011Speed: 116.58 samples/sec#011loss=1.940471\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784452.6334653, \"EndTime\": 1712784458.8024087, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6168.4887409210205, \"count\": 1, \"min\": 6168.4887409210205, \"max\": 6168.4887409210205}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.31715685115005 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=111, train loss <loss>=1.9551523382013494\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:38 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_2362f7da-e44a-4da8-8e5f-af8b28c53521-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784458.8024766, \"EndTime\": 1712784458.8834167, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 80.60693740844727, \"count\": 1, \"min\": 80.60693740844727, \"max\": 80.60693740844727}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:39 INFO 139627334805312] Epoch[112] Batch[0] avg_epoch_loss=1.900183\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=1.9001826047897339\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:42 INFO 139627334805312] Epoch[112] Batch[5] avg_epoch_loss=1.926562\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=1.9265621105829875\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:42 INFO 139627334805312] Epoch[112] Batch [5]#011Speed: 119.28 samples/sec#011loss=1.926562\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:44 INFO 139627334805312] Epoch[112] Batch[10] avg_epoch_loss=1.938348\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=1.9524914741516113\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:44 INFO 139627334805312] Epoch[112] Batch [10]#011Speed: 119.26 samples/sec#011loss=1.952491\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:44 INFO 139627334805312] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784458.8834844, \"EndTime\": 1712784464.9563494, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6072.807312011719, \"count\": 1, \"min\": 6072.807312011719, \"max\": 6072.807312011719}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:44 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.1494055854638 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:44 INFO 139627334805312] #progress_metric: host=algo-1, completed 56.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=112, train loss <loss>=1.938348184932362\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:44 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:45 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_2f9c6865-414d-4e3e-92db-ee81ec407227-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784464.9564183, \"EndTime\": 1712784465.0348995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.15432548522949, \"count\": 1, \"min\": 78.15432548522949, \"max\": 78.15432548522949}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:45 INFO 139627334805312] Epoch[113] Batch[0] avg_epoch_loss=1.943503\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=1.9435029029846191\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:48 INFO 139627334805312] Epoch[113] Batch[5] avg_epoch_loss=1.974451\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=1.9744511047999065\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:48 INFO 139627334805312] Epoch[113] Batch [5]#011Speed: 120.23 samples/sec#011loss=1.974451\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] Epoch[113] Batch[10] avg_epoch_loss=1.965453\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=1.9546555995941162\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] Epoch[113] Batch [10]#011Speed: 118.97 samples/sec#011loss=1.954656\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784465.0349655, \"EndTime\": 1712784471.0938265, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6058.8059425354, \"count\": 1, \"min\": 6058.8059425354, \"max\": 6058.8059425354}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.27046325185127 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=113, train loss <loss>=1.9654531478881836\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] Epoch[114] Batch[0] avg_epoch_loss=1.994217\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=1.9942171573638916\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:54 INFO 139627334805312] Epoch[114] Batch[5] avg_epoch_loss=1.970094\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=1.9700936675071716\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:54 INFO 139627334805312] Epoch[114] Batch [5]#011Speed: 119.30 samples/sec#011loss=1.970094\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:56 INFO 139627334805312] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784471.0938916, \"EndTime\": 1712784476.624746, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5530.458450317383, \"count\": 1, \"min\": 5530.458450317383, \"max\": 5530.458450317383}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:56 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.72070487707398 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:56 INFO 139627334805312] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=114, train loss <loss>=1.9726094245910644\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:56 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:57 INFO 139627334805312] Epoch[115] Batch[0] avg_epoch_loss=1.950306\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:27:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=1.9503060579299927\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:28:00 INFO 139627334805312] Epoch[115] Batch[5] avg_epoch_loss=1.981423\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=1.9814234574635823\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:00 INFO 139627334805312] Epoch[115] Batch [5]#011Speed: 119.59 samples/sec#011loss=1.981423\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:02 INFO 139627334805312] Epoch[115] Batch[10] avg_epoch_loss=1.991652\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=2.003927206993103\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:02 INFO 139627334805312] Epoch[115] Batch [10]#011Speed: 110.92 samples/sec#011loss=2.003927\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:02 INFO 139627334805312] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784476.624813, \"EndTime\": 1712784482.90633, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6281.1174392700195, \"count\": 1, \"min\": 6281.1174392700195, \"max\": 6281.1174392700195}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:02 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=104.59716976373096 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:02 INFO 139627334805312] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=115, train loss <loss>=1.9916524345224553\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:02 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:03 INFO 139627334805312] Epoch[116] Batch[0] avg_epoch_loss=2.004172\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=2.0041723251342773\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:06 INFO 139627334805312] Epoch[116] Batch[5] avg_epoch_loss=1.982641\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:06 INFO 139627334805312] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=1.9826412002245586\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:06 INFO 139627334805312] Epoch[116] Batch [5]#011Speed: 118.93 samples/sec#011loss=1.982641\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] Epoch[116] Batch[10] avg_epoch_loss=1.985805\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=1.9896004676818848\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] Epoch[116] Batch [10]#011Speed: 118.09 samples/sec#011loss=1.989600\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784482.9064233, \"EndTime\": 1712784489.0986342, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6191.72215461731, \"count\": 1, \"min\": 6191.72215461731, \"max\": 6191.72215461731}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.30683008261644 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] #progress_metric: host=algo-1, completed 58.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=116, train loss <loss>=1.9858045036142522\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] Epoch[117] Batch[0] avg_epoch_loss=1.924949\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=1.924948811531067\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:12 INFO 139627334805312] Epoch[117] Batch[5] avg_epoch_loss=1.954947\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=1.9549474517504375\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:12 INFO 139627334805312] Epoch[117] Batch [5]#011Speed: 119.47 samples/sec#011loss=1.954947\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] Epoch[117] Batch[10] avg_epoch_loss=1.993811\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=2.0404481649398805\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] Epoch[117] Batch [10]#011Speed: 119.44 samples/sec#011loss=2.040448\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784489.0986996, \"EndTime\": 1712784495.1775541, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6078.555107116699, \"count\": 1, \"min\": 6078.555107116699, \"max\": 6078.555107116699}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.273559458913 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=117, train loss <loss>=1.9938114122910933\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] Epoch[118] Batch[0] avg_epoch_loss=1.931165\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=1.931165337562561\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:18 INFO 139627334805312] Epoch[118] Batch[5] avg_epoch_loss=2.016172\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=2.0161717732747397\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:18 INFO 139627334805312] Epoch[118] Batch [5]#011Speed: 120.14 samples/sec#011loss=2.016172\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] Epoch[118] Batch[10] avg_epoch_loss=2.004760\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=1.9910651683807372\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] Epoch[118] Batch [10]#011Speed: 119.75 samples/sec#011loss=1.991065\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784495.1776206, \"EndTime\": 1712784501.219202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6041.298389434814, \"count\": 1, \"min\": 6041.298389434814, \"max\": 6041.298389434814}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.23262828481042 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] #progress_metric: host=algo-1, completed 59.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=118, train loss <loss>=2.004759680141102\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] Epoch[119] Batch[0] avg_epoch_loss=1.932253\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=1.932253122329712\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:24 INFO 139627334805312] Epoch[119] Batch[5] avg_epoch_loss=2.019880\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=2.0198804338773093\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:24 INFO 139627334805312] Epoch[119] Batch [5]#011Speed: 120.87 samples/sec#011loss=2.019880\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] Epoch[119] Batch[10] avg_epoch_loss=2.012055\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=2.002664566040039\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] Epoch[119] Batch [10]#011Speed: 120.19 samples/sec#011loss=2.002665\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784501.219267, \"EndTime\": 1712784507.2349515, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6015.383720397949, \"count\": 1, \"min\": 6015.383720397949, \"max\": 6015.383720397949}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.55589820638619 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=119, train loss <loss>=2.0120550394058228\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] Epoch[120] Batch[0] avg_epoch_loss=1.998811\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=1.99881112575531\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:28:30 INFO 139627334805312] Epoch[120] Batch[5] avg_epoch_loss=1.995259\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=1.9952585299809773\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:30 INFO 139627334805312] Epoch[120] Batch [5]#011Speed: 120.75 samples/sec#011loss=1.995259\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:32 INFO 139627334805312] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784507.2350144, \"EndTime\": 1712784512.73735, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5502.046823501587, \"count\": 1, \"min\": 5502.046823501587, \"max\": 5502.046823501587}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:32 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.95465595282856 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:32 INFO 139627334805312] #progress_metric: host=algo-1, completed 60.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=120, train loss <loss>=1.984849202632904\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:32 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:33 INFO 139627334805312] Epoch[121] Batch[0] avg_epoch_loss=1.944993\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=1.944993495941162\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:36 INFO 139627334805312] Epoch[121] Batch[5] avg_epoch_loss=1.944853\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=1.9448528091112773\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:36 INFO 139627334805312] Epoch[121] Batch [5]#011Speed: 120.89 samples/sec#011loss=1.944853\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:38 INFO 139627334805312] Epoch[121] Batch[10] avg_epoch_loss=1.961957\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=1.9824826002120972\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:38 INFO 139627334805312] Epoch[121] Batch [10]#011Speed: 116.04 samples/sec#011loss=1.982483\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:38 INFO 139627334805312] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784512.7374213, \"EndTime\": 1712784518.841669, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6103.800535202026, \"count\": 1, \"min\": 6103.800535202026, \"max\": 6103.800535202026}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:38 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.12685646227962 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:38 INFO 139627334805312] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=121, train loss <loss>=1.96195725961165\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:38 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:39 INFO 139627334805312] Epoch[122] Batch[0] avg_epoch_loss=1.933155\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=1.9331549406051636\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:42 INFO 139627334805312] Epoch[122] Batch[5] avg_epoch_loss=1.946266\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=1.9462659358978271\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:42 INFO 139627334805312] Epoch[122] Batch [5]#011Speed: 118.01 samples/sec#011loss=1.946266\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] Epoch[122] Batch[10] avg_epoch_loss=1.989579\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=2.041555619239807\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] Epoch[122] Batch [10]#011Speed: 116.10 samples/sec#011loss=2.041556\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784518.8417776, \"EndTime\": 1712784525.0357187, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6193.628549575806, \"count\": 1, \"min\": 6193.628549575806, \"max\": 6193.628549575806}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.26778107547746 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] #progress_metric: host=algo-1, completed 61.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=122, train loss <loss>=1.9895794283259998\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] Epoch[123] Batch[0] avg_epoch_loss=1.926945\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=1.9269447326660156\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:48 INFO 139627334805312] Epoch[123] Batch[5] avg_epoch_loss=1.945415\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=1.9454153180122375\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:48 INFO 139627334805312] Epoch[123] Batch [5]#011Speed: 116.63 samples/sec#011loss=1.945415\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] Epoch[123] Batch[10] avg_epoch_loss=1.991535\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=2.046877646446228\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] Epoch[123] Batch [10]#011Speed: 117.56 samples/sec#011loss=2.046878\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784525.035787, \"EndTime\": 1712784531.235794, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6199.714660644531, \"count\": 1, \"min\": 6199.714660644531, \"max\": 6199.714660644531}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=103.3902249578121 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=123, train loss <loss>=1.9915345582095059\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] Epoch[124] Batch[0] avg_epoch_loss=1.929391\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=1.929390788078308\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:54 INFO 139627334805312] Epoch[124] Batch[5] avg_epoch_loss=1.990325\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=1.9903252124786377\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:54 INFO 139627334805312] Epoch[124] Batch [5]#011Speed: 118.25 samples/sec#011loss=1.990325\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:56 INFO 139627334805312] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784531.2358613, \"EndTime\": 1712784536.8222628, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5586.108922958374, \"count\": 1, \"min\": 5586.108922958374, \"max\": 5586.108922958374}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:56 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.38667212047918 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:56 INFO 139627334805312] #progress_metric: host=algo-1, completed 62.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=124, train loss <loss>=1.9783066391944886\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:56 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:57 INFO 139627334805312] Epoch[125] Batch[0] avg_epoch_loss=1.911395\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:28:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=1.9113950729370117\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:00 INFO 139627334805312] Epoch[125] Batch[5] avg_epoch_loss=1.954719\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=1.9547187089920044\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:00 INFO 139627334805312] Epoch[125] Batch [5]#011Speed: 119.93 samples/sec#011loss=1.954719\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:03 INFO 139627334805312] Epoch[125] Batch[10] avg_epoch_loss=1.971063\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=1.9906758546829224\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:03 INFO 139627334805312] Epoch[125] Batch [10]#011Speed: 101.43 samples/sec#011loss=1.990676\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:03 INFO 139627334805312] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784536.8224046, \"EndTime\": 1712784543.3502307, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6527.363300323486, \"count\": 1, \"min\": 6527.363300323486, \"max\": 6527.363300323486}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:03 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=101.26447299599388 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:03 INFO 139627334805312] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=125, train loss <loss>=1.9710628661242398\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:03 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:04 INFO 139627334805312] Epoch[126] Batch[0] avg_epoch_loss=1.925874\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=1.9258735179901123\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:07 INFO 139627334805312] Epoch[126] Batch[5] avg_epoch_loss=1.964759\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=1.964758574962616\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:07 INFO 139627334805312] Epoch[126] Batch [5]#011Speed: 110.84 samples/sec#011loss=1.964759\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:09 INFO 139627334805312] Epoch[126] Batch[10] avg_epoch_loss=1.949643\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=1.931503701210022\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:09 INFO 139627334805312] Epoch[126] Batch [10]#011Speed: 111.56 samples/sec#011loss=1.931504\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:09 INFO 139627334805312] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784543.3502975, \"EndTime\": 1712784549.8948774, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6544.223308563232, \"count\": 1, \"min\": 6544.223308563232, \"max\": 6544.223308563232}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:09 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=101.61480789285535 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:09 INFO 139627334805312] #progress_metric: host=algo-1, completed 63.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=126, train loss <loss>=1.9496427232568914\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:09 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:10 INFO 139627334805312] Epoch[127] Batch[0] avg_epoch_loss=1.952140\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=1.9521396160125732\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:29:13 INFO 139627334805312] Epoch[127] Batch[5] avg_epoch_loss=1.974418\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=1.9744176467259724\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:13 INFO 139627334805312] Epoch[127] Batch [5]#011Speed: 111.96 samples/sec#011loss=1.974418\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:16 INFO 139627334805312] Epoch[127] Batch[10] avg_epoch_loss=1.989213\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=2.0069676876068114\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:16 INFO 139627334805312] Epoch[127] Batch [10]#011Speed: 111.63 samples/sec#011loss=2.006968\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:16 INFO 139627334805312] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784549.8949444, \"EndTime\": 1712784556.3718376, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6476.590871810913, \"count\": 1, \"min\": 6476.590871810913, \"max\": 6476.590871810913}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:16 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=100.20553119432215 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:16 INFO 139627334805312] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=127, train loss <loss>=1.9892131198536267\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:16 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:17 INFO 139627334805312] Epoch[128] Batch[0] avg_epoch_loss=2.072745\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=2.072744846343994\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:19 INFO 139627334805312] Epoch[128] Batch[5] avg_epoch_loss=1.977888\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=1.9778875311215718\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:19 INFO 139627334805312] Epoch[128] Batch [5]#011Speed: 112.13 samples/sec#011loss=1.977888\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:22 INFO 139627334805312] processed a total of 582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784556.3719044, \"EndTime\": 1712784562.269335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5897.12929725647, \"count\": 1, \"min\": 5897.12929725647, \"max\": 5897.12929725647}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:22 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=98.69020810497783 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:22 INFO 139627334805312] #progress_metric: host=algo-1, completed 64.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:22 INFO 139627334805312] #quality_metric: host=algo-1, epoch=128, train loss <loss>=1.9444498419761658\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:22 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:23 INFO 139627334805312] Epoch[129] Batch[0] avg_epoch_loss=1.877265\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=1.877265214920044\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:25 INFO 139627334805312] Epoch[129] Batch[5] avg_epoch_loss=1.931294\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=1.9312941233317058\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:25 INFO 139627334805312] Epoch[129] Batch [5]#011Speed: 114.29 samples/sec#011loss=1.931294\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:27 INFO 139627334805312] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784562.269412, \"EndTime\": 1712784567.9835207, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5713.547468185425, \"count\": 1, \"min\": 5713.547468185425, \"max\": 5713.547468185425}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:27 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.7618373852911 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:27 INFO 139627334805312] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=129, train loss <loss>=1.9415416479110719\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:27 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:28 INFO 139627334805312] Epoch[130] Batch[0] avg_epoch_loss=1.905461\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:28 INFO 139627334805312] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=1.9054614305496216\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:31 INFO 139627334805312] Epoch[130] Batch[5] avg_epoch_loss=1.969425\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=1.969424804051717\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:31 INFO 139627334805312] Epoch[130] Batch [5]#011Speed: 119.43 samples/sec#011loss=1.969425\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] Epoch[130] Batch[10] avg_epoch_loss=1.951672\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=1.9303686141967773\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] Epoch[130] Batch [10]#011Speed: 118.44 samples/sec#011loss=1.930369\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784567.983592, \"EndTime\": 1712784574.0776389, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6093.676805496216, \"count\": 1, \"min\": 6093.676805496216, \"max\": 6093.676805496216}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.32273680303794 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] #progress_metric: host=algo-1, completed 65.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=130, train loss <loss>=1.95167199048129\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] Epoch[131] Batch[0] avg_epoch_loss=1.987098\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=1.987098217010498\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:37 INFO 139627334805312] Epoch[131] Batch[5] avg_epoch_loss=1.940176\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=1.9401764869689941\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:37 INFO 139627334805312] Epoch[131] Batch [5]#011Speed: 118.26 samples/sec#011loss=1.940176\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] Epoch[131] Batch[10] avg_epoch_loss=1.952904\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=1.968176507949829\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] Epoch[131] Batch [10]#011Speed: 120.17 samples/sec#011loss=1.968177\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784574.0777018, \"EndTime\": 1712784580.1553855, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6077.390193939209, \"count\": 1, \"min\": 6077.390193939209, \"max\": 6077.390193939209}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.9265453423055 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=131, train loss <loss>=1.95290376923301\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] Epoch[132] Batch[0] avg_epoch_loss=1.937943\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=1.9379433393478394\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:43 INFO 139627334805312] Epoch[132] Batch[5] avg_epoch_loss=1.937879\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=1.9378789067268372\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:43 INFO 139627334805312] Epoch[132] Batch [5]#011Speed: 118.97 samples/sec#011loss=1.937879\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:45 INFO 139627334805312] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784580.1554554, \"EndTime\": 1712784585.7021847, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5546.3409423828125, \"count\": 1, \"min\": 5546.3409423828125, \"max\": 5546.3409423828125}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:45 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.63664937530538 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:45 INFO 139627334805312] #progress_metric: host=algo-1, completed 66.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=132, train loss <loss>=1.959369146823883\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:45 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:46 INFO 139627334805312] Epoch[133] Batch[0] avg_epoch_loss=1.855526\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=1.8555258512496948\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:49 INFO 139627334805312] Epoch[133] Batch[5] avg_epoch_loss=1.886560\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=1.8865599036216736\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:49 INFO 139627334805312] Epoch[133] Batch [5]#011Speed: 120.45 samples/sec#011loss=1.886560\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] Epoch[133] Batch[10] avg_epoch_loss=1.910400\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=1.939007616043091\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] Epoch[133] Batch [10]#011Speed: 118.18 samples/sec#011loss=1.939008\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784585.702248, \"EndTime\": 1712784591.776458, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6073.730230331421, \"count\": 1, \"min\": 6073.730230331421, \"max\": 6073.730230331421}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.67513644791886 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=133, train loss <loss>=1.910399772904136\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:51 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_99bba904-942f-4ed0-b318-3676d865664d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784591.776522, \"EndTime\": 1712784591.8538551, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.00324058532715, \"count\": 1, \"min\": 77.00324058532715, \"max\": 77.00324058532715}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:52 INFO 139627334805312] Epoch[134] Batch[0] avg_epoch_loss=1.919703\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=1.919703483581543\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:29:55 INFO 139627334805312] Epoch[134] Batch[5] avg_epoch_loss=1.907251\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=1.9072506825129192\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:55 INFO 139627334805312] Epoch[134] Batch [5]#011Speed: 119.79 samples/sec#011loss=1.907251\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:57 INFO 139627334805312] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784591.8539252, \"EndTime\": 1712784597.3931775, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5539.191961288452, \"count\": 1, \"min\": 5539.191961288452, \"max\": 5539.191961288452}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:57 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.9123353841038 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:57 INFO 139627334805312] #progress_metric: host=algo-1, completed 67.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=134, train loss <loss>=1.9155466198921203\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:57 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:58 INFO 139627334805312] Epoch[135] Batch[0] avg_epoch_loss=1.959288\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:29:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=1.9592878818511963\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:00 INFO 139627334805312] Epoch[135] Batch[5] avg_epoch_loss=1.906831\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=1.9068312048912048\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:00 INFO 139627334805312] Epoch[135] Batch [5]#011Speed: 119.15 samples/sec#011loss=1.906831\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:03 INFO 139627334805312] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784597.3932981, \"EndTime\": 1712784603.1872442, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5793.60032081604, \"count\": 1, \"min\": 5793.60032081604, \"max\": 5793.60032081604}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:03 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=103.04288449026086 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:03 INFO 139627334805312] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=135, train loss <loss>=1.9003959894180298\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:03 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:03 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_46ed82f8-36e7-4105-9e63-ee9bf0168ed0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784603.187316, \"EndTime\": 1712784603.2659965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.2465934753418, \"count\": 1, \"min\": 78.2465934753418, \"max\": 78.2465934753418}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:04 INFO 139627334805312] Epoch[136] Batch[0] avg_epoch_loss=1.830113\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=1.8301125764846802\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:06 INFO 139627334805312] Epoch[136] Batch[5] avg_epoch_loss=1.895615\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:06 INFO 139627334805312] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=1.8956148227055867\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:06 INFO 139627334805312] Epoch[136] Batch [5]#011Speed: 120.02 samples/sec#011loss=1.895615\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:09 INFO 139627334805312] Epoch[136] Batch[10] avg_epoch_loss=1.908291\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=1.9235026121139527\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:09 INFO 139627334805312] Epoch[136] Batch [10]#011Speed: 118.50 samples/sec#011loss=1.923503\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:09 INFO 139627334805312] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784603.2660651, \"EndTime\": 1712784609.4015665, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6135.444402694702, \"count\": 1, \"min\": 6135.444402694702, \"max\": 6135.444402694702}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:09 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.36281841814213 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:09 INFO 139627334805312] #progress_metric: host=algo-1, completed 68.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=136, train loss <loss>=1.9082910906184802\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:09 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:10 INFO 139627334805312] Epoch[137] Batch[0] avg_epoch_loss=1.923598\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=1.9235979318618774\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:12 INFO 139627334805312] Epoch[137] Batch[5] avg_epoch_loss=1.906999\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=1.906998833020528\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:12 INFO 139627334805312] Epoch[137] Batch [5]#011Speed: 118.92 samples/sec#011loss=1.906999\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:15 INFO 139627334805312] Epoch[137] Batch[10] avg_epoch_loss=1.947783\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=1.9967241287231445\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:15 INFO 139627334805312] Epoch[137] Batch [10]#011Speed: 117.76 samples/sec#011loss=1.996724\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:15 INFO 139627334805312] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784609.4016309, \"EndTime\": 1712784615.5190532, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6117.126703262329, \"count\": 1, \"min\": 6117.126703262329, \"max\": 6117.126703262329}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:15 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.12325201606845 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:15 INFO 139627334805312] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=137, train loss <loss>=1.9477830583398992\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:15 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:16 INFO 139627334805312] Epoch[138] Batch[0] avg_epoch_loss=1.937840\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=1.9378395080566406\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:18 INFO 139627334805312] Epoch[138] Batch[5] avg_epoch_loss=1.936832\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=1.9368316332499187\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:18 INFO 139627334805312] Epoch[138] Batch [5]#011Speed: 120.21 samples/sec#011loss=1.936832\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:21 INFO 139627334805312] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784615.5191178, \"EndTime\": 1712784621.0221584, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5502.747535705566, \"count\": 1, \"min\": 5502.747535705566, \"max\": 5502.747535705566}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:21 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.21398217605905 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:21 INFO 139627334805312] #progress_metric: host=algo-1, completed 69.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=138, train loss <loss>=1.9276952743530273\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:21 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:21 INFO 139627334805312] Epoch[139] Batch[0] avg_epoch_loss=1.851519\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=1.851518988609314\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:30:24 INFO 139627334805312] Epoch[139] Batch[5] avg_epoch_loss=1.927517\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=1.9275170763333638\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:24 INFO 139627334805312] Epoch[139] Batch [5]#011Speed: 120.12 samples/sec#011loss=1.927517\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] Epoch[139] Batch[10] avg_epoch_loss=1.912477\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=1.8944287538528441\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] Epoch[139] Batch [10]#011Speed: 118.74 samples/sec#011loss=1.894429\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784621.0222313, \"EndTime\": 1712784627.086172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6063.516855239868, \"count\": 1, \"min\": 6063.516855239868, \"max\": 6063.516855239868}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.16396569584485 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=139, train loss <loss>=1.9124769297513096\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] Epoch[140] Batch[0] avg_epoch_loss=1.902416\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=1.9024158716201782\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:30 INFO 139627334805312] Epoch[140] Batch[5] avg_epoch_loss=1.920789\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=1.9207887053489685\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:30 INFO 139627334805312] Epoch[140] Batch [5]#011Speed: 120.94 samples/sec#011loss=1.920789\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] Epoch[140] Batch[10] avg_epoch_loss=1.914604\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=1.9071827411651612\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] Epoch[140] Batch [10]#011Speed: 118.23 samples/sec#011loss=1.907183\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784627.0863063, \"EndTime\": 1712784633.1466222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6059.836626052856, \"count\": 1, \"min\": 6059.836626052856, \"max\": 6059.836626052856}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.5819826315381 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] #progress_metric: host=algo-1, completed 70.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=140, train loss <loss>=1.9146041761745105\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] Epoch[141] Batch[0] avg_epoch_loss=1.981590\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=1.9815901517868042\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:36 INFO 139627334805312] Epoch[141] Batch[5] avg_epoch_loss=1.930502\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=1.9305022756258647\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:36 INFO 139627334805312] Epoch[141] Batch [5]#011Speed: 118.67 samples/sec#011loss=1.930502\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:39 INFO 139627334805312] Epoch[141] Batch[10] avg_epoch_loss=1.949604\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=1.97252676486969\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:39 INFO 139627334805312] Epoch[141] Batch [10]#011Speed: 115.58 samples/sec#011loss=1.972527\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:39 INFO 139627334805312] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784633.146689, \"EndTime\": 1712784639.3137157, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6166.612863540649, \"count\": 1, \"min\": 6166.612863540649, \"max\": 6166.612863540649}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:39 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.56680696280088 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:39 INFO 139627334805312] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=141, train loss <loss>=1.9496043161912398\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:39 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:40 INFO 139627334805312] Epoch[142] Batch[0] avg_epoch_loss=1.919667\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=1.9196666479110718\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:42 INFO 139627334805312] Epoch[142] Batch[5] avg_epoch_loss=1.933156\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=1.9331562916437786\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:42 INFO 139627334805312] Epoch[142] Batch [5]#011Speed: 118.94 samples/sec#011loss=1.933156\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:45 INFO 139627334805312] Epoch[142] Batch[10] avg_epoch_loss=1.907055\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=1.8757330894470214\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:45 INFO 139627334805312] Epoch[142] Batch [10]#011Speed: 118.95 samples/sec#011loss=1.875733\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:45 INFO 139627334805312] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784639.3137803, \"EndTime\": 1712784645.4024293, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6088.329553604126, \"count\": 1, \"min\": 6088.329553604126, \"max\": 6088.329553604126}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:45 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.56647127898142 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:45 INFO 139627334805312] #progress_metric: host=algo-1, completed 71.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=142, train loss <loss>=1.907054836099798\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:45 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:46 INFO 139627334805312] Epoch[143] Batch[0] avg_epoch_loss=2.005851\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=2.0058507919311523\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:48 INFO 139627334805312] Epoch[143] Batch[5] avg_epoch_loss=1.961437\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=1.9614367882410686\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:48 INFO 139627334805312] Epoch[143] Batch [5]#011Speed: 117.27 samples/sec#011loss=1.961437\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:51 INFO 139627334805312] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784645.4025037, \"EndTime\": 1712784651.045459, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5642.555475234985, \"count\": 1, \"min\": 5642.555475234985, \"max\": 5642.555475234985}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:51 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.06690887067242 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:51 INFO 139627334805312] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=143, train loss <loss>=1.9581888437271118\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:51 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:51 INFO 139627334805312] Epoch[144] Batch[0] avg_epoch_loss=1.904040\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=1.9040404558181763\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:30:54 INFO 139627334805312] Epoch[144] Batch[5] avg_epoch_loss=1.931889\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=1.931889255841573\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:54 INFO 139627334805312] Epoch[144] Batch [5]#011Speed: 119.01 samples/sec#011loss=1.931889\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] Epoch[144] Batch[10] avg_epoch_loss=1.923401\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=1.913214421272278\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] Epoch[144] Batch [10]#011Speed: 119.38 samples/sec#011loss=1.913214\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784651.0455422, \"EndTime\": 1712784657.1357083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6089.715957641602, \"count\": 1, \"min\": 6089.715957641602, \"max\": 6089.715957641602}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.42196879212094 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] #progress_metric: host=algo-1, completed 72.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=144, train loss <loss>=1.9234006946737117\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] Epoch[145] Batch[0] avg_epoch_loss=1.898177\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:30:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=1.898176670074463\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:00 INFO 139627334805312] Epoch[145] Batch[5] avg_epoch_loss=1.872977\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=1.872976541519165\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:00 INFO 139627334805312] Epoch[145] Batch [5]#011Speed: 119.75 samples/sec#011loss=1.872977\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:03 INFO 139627334805312] Epoch[145] Batch[10] avg_epoch_loss=1.912852\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=1.9607025623321532\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:03 INFO 139627334805312] Epoch[145] Batch [10]#011Speed: 110.68 samples/sec#011loss=1.960703\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:03 INFO 139627334805312] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784657.135774, \"EndTime\": 1712784663.4215074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6285.438537597656, \"count\": 1, \"min\": 6285.438537597656, \"max\": 6285.438537597656}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:03 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=104.52574666389262 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:03 INFO 139627334805312] #progress_metric: host=algo-1, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=145, train loss <loss>=1.9128520055250688\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:03 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:04 INFO 139627334805312] Epoch[146] Batch[0] avg_epoch_loss=1.870294\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=1.8702940940856934\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:06 INFO 139627334805312] Epoch[146] Batch[5] avg_epoch_loss=1.904324\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:06 INFO 139627334805312] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=1.904324233531952\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:06 INFO 139627334805312] Epoch[146] Batch [5]#011Speed: 118.34 samples/sec#011loss=1.904324\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:09 INFO 139627334805312] Epoch[146] Batch[10] avg_epoch_loss=1.922630\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=1.9445971727371216\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:09 INFO 139627334805312] Epoch[146] Batch [10]#011Speed: 119.50 samples/sec#011loss=1.944597\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:09 INFO 139627334805312] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784663.4215717, \"EndTime\": 1712784669.5284288, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6106.564521789551, \"count\": 1, \"min\": 6106.564521789551, \"max\": 6106.564521789551}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:09 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.22500352357515 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:09 INFO 139627334805312] #progress_metric: host=algo-1, completed 73.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=146, train loss <loss>=1.9226301149888472\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:09 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:10 INFO 139627334805312] Epoch[147] Batch[0] avg_epoch_loss=1.912755\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=1.912754774093628\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:12 INFO 139627334805312] Epoch[147] Batch[5] avg_epoch_loss=1.906865\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=1.9068646430969238\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:12 INFO 139627334805312] Epoch[147] Batch [5]#011Speed: 119.50 samples/sec#011loss=1.906865\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:14 INFO 139627334805312] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784669.528495, \"EndTime\": 1712784674.5119941, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4983.197927474976, \"count\": 1, \"min\": 4983.197927474976, \"max\": 4983.197927474976}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:14 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.58608576334584 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:14 INFO 139627334805312] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=147, train loss <loss>=1.9088920487297907\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:14 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:15 INFO 139627334805312] Epoch[148] Batch[0] avg_epoch_loss=1.876444\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=1.876443862915039\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:17 INFO 139627334805312] Epoch[148] Batch[5] avg_epoch_loss=1.873849\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=1.8738485773404439\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:17 INFO 139627334805312] Epoch[148] Batch [5]#011Speed: 120.41 samples/sec#011loss=1.873849\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:19 INFO 139627334805312] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784674.512063, \"EndTime\": 1712784679.9991918, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5486.783981323242, \"count\": 1, \"min\": 5486.783981323242, \"max\": 5486.783981323242}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:19 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.73044324902969 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:19 INFO 139627334805312] #progress_metric: host=algo-1, completed 74.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=148, train loss <loss>=1.8827709078788757\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:19 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:20 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_a010f80f-d665-43c6-b314-ddb891c10e52-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784679.9992626, \"EndTime\": 1712784680.077547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.91256904602051, \"count\": 1, \"min\": 77.91256904602051, \"max\": 77.91256904602051}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:20 INFO 139627334805312] Epoch[149] Batch[0] avg_epoch_loss=1.823384\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=1.8233840465545654\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:31:23 INFO 139627334805312] Epoch[149] Batch[5] avg_epoch_loss=1.865807\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=1.8658069173494976\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:23 INFO 139627334805312] Epoch[149] Batch [5]#011Speed: 119.65 samples/sec#011loss=1.865807\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:25 INFO 139627334805312] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784680.0776155, \"EndTime\": 1712784685.5910249, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5513.348817825317, \"count\": 1, \"min\": 5513.348817825317, \"max\": 5513.348817825317}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:25 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.81005369748824 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:25 INFO 139627334805312] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=149, train loss <loss>=1.8753512501716614\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:25 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:25 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_830bf517-02a6-4127-9898-ff359721b205-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784685.591096, \"EndTime\": 1712784685.6694708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.01318168640137, \"count\": 1, \"min\": 78.01318168640137, \"max\": 78.01318168640137}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:26 INFO 139627334805312] Epoch[150] Batch[0] avg_epoch_loss=1.869913\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=1.8699134588241577\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:29 INFO 139627334805312] Epoch[150] Batch[5] avg_epoch_loss=1.899187\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=1.8991874655087788\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:29 INFO 139627334805312] Epoch[150] Batch [5]#011Speed: 117.68 samples/sec#011loss=1.899187\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:31 INFO 139627334805312] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784685.6695383, \"EndTime\": 1712784691.2973065, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5627.70938873291, \"count\": 1, \"min\": 5627.70938873291, \"max\": 5627.70938873291}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:31 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.36554175777118 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:31 INFO 139627334805312] #progress_metric: host=algo-1, completed 75.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=150, train loss <loss>=1.8946473121643066\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:31 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:32 INFO 139627334805312] Epoch[151] Batch[0] avg_epoch_loss=1.904368\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=1.9043678045272827\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:34 INFO 139627334805312] Epoch[151] Batch[5] avg_epoch_loss=1.881584\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=1.8815836509068806\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:34 INFO 139627334805312] Epoch[151] Batch [5]#011Speed: 120.10 samples/sec#011loss=1.881584\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:36 INFO 139627334805312] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784691.297375, \"EndTime\": 1712784696.814518, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5516.712665557861, \"count\": 1, \"min\": 5516.712665557861, \"max\": 5516.712665557861}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:36 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.20234948343536 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:36 INFO 139627334805312] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=151, train loss <loss>=1.878450298309326\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:36 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:37 INFO 139627334805312] Epoch[152] Batch[0] avg_epoch_loss=1.826830\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=1.8268296718597412\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:40 INFO 139627334805312] Epoch[152] Batch[5] avg_epoch_loss=1.884720\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=1.8847198883692424\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:40 INFO 139627334805312] Epoch[152] Batch [5]#011Speed: 118.31 samples/sec#011loss=1.884720\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:42 INFO 139627334805312] Epoch[152] Batch[10] avg_epoch_loss=1.891639\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=1.8999412059783936\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:42 INFO 139627334805312] Epoch[152] Batch [10]#011Speed: 118.68 samples/sec#011loss=1.899941\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:42 INFO 139627334805312] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784696.8145888, \"EndTime\": 1712784702.9286842, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6113.686084747314, \"count\": 1, \"min\": 6113.686084747314, \"max\": 6113.686084747314}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:42 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=105.82644767102157 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:42 INFO 139627334805312] #progress_metric: host=algo-1, completed 76.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=152, train loss <loss>=1.8916386691006748\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:42 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:43 INFO 139627334805312] Epoch[153] Batch[0] avg_epoch_loss=2.001517\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=2.0015172958374023\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:46 INFO 139627334805312] Epoch[153] Batch[5] avg_epoch_loss=1.914101\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=1.9141006271044414\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:46 INFO 139627334805312] Epoch[153] Batch [5]#011Speed: 118.02 samples/sec#011loss=1.914101\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] Epoch[153] Batch[10] avg_epoch_loss=1.867652\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=1.8119145631790161\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] Epoch[153] Batch [10]#011Speed: 120.01 samples/sec#011loss=1.811915\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784702.9287505, \"EndTime\": 1712784709.0151615, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6086.101770401001, \"count\": 1, \"min\": 6086.101770401001, \"max\": 6086.101770401001}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.94923655994347 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] #progress_metric: host=algo-1, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=153, train loss <loss>=1.867652416229248\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_ab7be45a-d968-42a4-a96b-b6337070e795-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784709.0152252, \"EndTime\": 1712784709.092324, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 76.72739028930664, \"count\": 1, \"min\": 76.72739028930664, \"max\": 76.72739028930664}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] Epoch[154] Batch[0] avg_epoch_loss=1.907315\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=1.9073147773742676\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:52 INFO 139627334805312] Epoch[154] Batch[5] avg_epoch_loss=1.919900\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=1.9199004968007405\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:52 INFO 139627334805312] Epoch[154] Batch [5]#011Speed: 119.33 samples/sec#011loss=1.919900\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:54 INFO 139627334805312] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784709.0923903, \"EndTime\": 1712784714.6292908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5536.841869354248, \"count\": 1, \"min\": 5536.841869354248, \"max\": 5536.841869354248}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:54 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.62720989826012 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:54 INFO 139627334805312] #progress_metric: host=algo-1, completed 77.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=154, train loss <loss>=1.9326231241226197\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:54 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:55 INFO 139627334805312] Epoch[155] Batch[0] avg_epoch_loss=1.851755\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=1.8517554998397827\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:31:58 INFO 139627334805312] Epoch[155] Batch[5] avg_epoch_loss=1.924369\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=1.92436945438385\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:31:58 INFO 139627334805312] Epoch[155] Batch [5]#011Speed: 118.59 samples/sec#011loss=1.924369\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:00 INFO 139627334805312] Epoch[155] Batch[10] avg_epoch_loss=1.943615\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=1.9667088747024537\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:00 INFO 139627334805312] Epoch[155] Batch [10]#011Speed: 119.74 samples/sec#011loss=1.966709\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:00 INFO 139627334805312] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784714.6293626, \"EndTime\": 1712784720.704038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6074.265718460083, \"count\": 1, \"min\": 6074.265718460083, \"max\": 6074.265718460083}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:00 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.48874206782591 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:00 INFO 139627334805312] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=155, train loss <loss>=1.9436146454377607\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:00 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:01 INFO 139627334805312] Epoch[156] Batch[0] avg_epoch_loss=1.942850\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=1.94284987449646\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:04 INFO 139627334805312] Epoch[156] Batch[5] avg_epoch_loss=1.904562\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=1.904561718304952\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:04 INFO 139627334805312] Epoch[156] Batch [5]#011Speed: 107.76 samples/sec#011loss=1.904562\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] Epoch[156] Batch[10] avg_epoch_loss=1.863520\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=1.8142699241638183\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] Epoch[156] Batch [10]#011Speed: 119.74 samples/sec#011loss=1.814270\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784720.7041047, \"EndTime\": 1712784727.069109, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6364.705562591553, \"count\": 1, \"min\": 6364.705562591553, \"max\": 6364.705562591553}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=101.6527800762978 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] #progress_metric: host=algo-1, completed 78.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=156, train loss <loss>=1.8635199936953457\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_87212838-5a6e-449d-ba23-bdb22fe02463-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784727.0691767, \"EndTime\": 1712784727.1482491, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 78.70626449584961, \"count\": 1, \"min\": 78.70626449584961, \"max\": 78.70626449584961}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] Epoch[157] Batch[0] avg_epoch_loss=1.876469\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=1.8764688968658447\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:10 INFO 139627334805312] Epoch[157] Batch[5] avg_epoch_loss=1.898945\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=1.8989446361859639\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:10 INFO 139627334805312] Epoch[157] Batch [5]#011Speed: 119.83 samples/sec#011loss=1.898945\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:13 INFO 139627334805312] Epoch[157] Batch[10] avg_epoch_loss=1.881283\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=1.860088086128235\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:13 INFO 139627334805312] Epoch[157] Batch [10]#011Speed: 116.86 samples/sec#011loss=1.860088\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:13 INFO 139627334805312] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784727.1483164, \"EndTime\": 1712784733.2684872, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6120.11194229126, \"count\": 1, \"min\": 6120.11194229126, \"max\": 6120.11194229126}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:13 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.53221866939899 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:13 INFO 139627334805312] #progress_metric: host=algo-1, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=157, train loss <loss>=1.8812825679779053\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:13 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:14 INFO 139627334805312] Epoch[158] Batch[0] avg_epoch_loss=1.890861\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=1.8908612728118896\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:16 INFO 139627334805312] Epoch[158] Batch[5] avg_epoch_loss=1.883634\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=1.8836336930592854\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:16 INFO 139627334805312] Epoch[158] Batch [5]#011Speed: 116.02 samples/sec#011loss=1.883634\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:18 INFO 139627334805312] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784733.2685568, \"EndTime\": 1712784738.926087, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5657.224178314209, \"count\": 1, \"min\": 5657.224178314209, \"max\": 5657.224178314209}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:18 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.29441163869743 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:18 INFO 139627334805312] #progress_metric: host=algo-1, completed 79.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=158, train loss <loss>=1.9166122317314147\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:18 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:19 INFO 139627334805312] Epoch[159] Batch[0] avg_epoch_loss=1.858701\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=1.8587009906768799\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:22 INFO 139627334805312] Epoch[159] Batch[5] avg_epoch_loss=1.879698\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:22 INFO 139627334805312] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=1.8796979983647664\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:22 INFO 139627334805312] Epoch[159] Batch [5]#011Speed: 120.18 samples/sec#011loss=1.879698\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:24 INFO 139627334805312] Epoch[159] Batch[10] avg_epoch_loss=1.853859\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=1.822852897644043\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:24 INFO 139627334805312] Epoch[159] Batch [10]#011Speed: 119.97 samples/sec#011loss=1.822853\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:24 INFO 139627334805312] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784738.9261608, \"EndTime\": 1712784744.9639475, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6037.410259246826, \"count\": 1, \"min\": 6037.410259246826, \"max\": 6037.410259246826}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:24 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.50091070854562 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:24 INFO 139627334805312] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=159, train loss <loss>=1.8538593162189831\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:24 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:25 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_8aa552e6-a18b-48d1-8830-34bd2b485305-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784744.9640145, \"EndTime\": 1712784745.0417655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.40664482116699, \"count\": 1, \"min\": 77.40664482116699, \"max\": 77.40664482116699}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:25 INFO 139627334805312] Epoch[160] Batch[0] avg_epoch_loss=1.965376\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=1.9653756618499756\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:32:28 INFO 139627334805312] Epoch[160] Batch[5] avg_epoch_loss=1.859329\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:28 INFO 139627334805312] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=1.8593294819196065\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:28 INFO 139627334805312] Epoch[160] Batch [5]#011Speed: 119.62 samples/sec#011loss=1.859329\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] Epoch[160] Batch[10] avg_epoch_loss=1.883483\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=1.9124661684036255\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] Epoch[160] Batch [10]#011Speed: 118.12 samples/sec#011loss=1.912466\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784745.0418313, \"EndTime\": 1712784751.1298943, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6088.006973266602, \"count\": 1, \"min\": 6088.006973266602, \"max\": 6088.006973266602}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.50008535227659 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] #progress_metric: host=algo-1, completed 80.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=160, train loss <loss>=1.8834825212305242\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] Epoch[161] Batch[0] avg_epoch_loss=1.856778\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=1.8567777872085571\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:34 INFO 139627334805312] Epoch[161] Batch[5] avg_epoch_loss=1.847937\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=1.8479368090629578\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:34 INFO 139627334805312] Epoch[161] Batch [5]#011Speed: 120.57 samples/sec#011loss=1.847937\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:36 INFO 139627334805312] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784751.1299582, \"EndTime\": 1712784756.6453722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5515.110015869141, \"count\": 1, \"min\": 5515.110015869141, \"max\": 5515.110015869141}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:36 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.31734560352128 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:36 INFO 139627334805312] #progress_metric: host=algo-1, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:36 INFO 139627334805312] #quality_metric: host=algo-1, epoch=161, train loss <loss>=1.8664824962615967\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:36 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:37 INFO 139627334805312] Epoch[162] Batch[0] avg_epoch_loss=1.862533\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=1.8625329732894897\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:40 INFO 139627334805312] Epoch[162] Batch[5] avg_epoch_loss=1.872867\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=1.8728668689727783\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:40 INFO 139627334805312] Epoch[162] Batch [5]#011Speed: 119.44 samples/sec#011loss=1.872867\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:42 INFO 139627334805312] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784756.6454446, \"EndTime\": 1712784762.1666203, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5520.777940750122, \"count\": 1, \"min\": 5520.777940750122, \"max\": 5520.777940750122}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:42 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.12729047075206 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:42 INFO 139627334805312] #progress_metric: host=algo-1, completed 81.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=162, train loss <loss>=1.850080955028534\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:42 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:42 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_e4ae8b27-87ab-4d36-aba6-c56b8fc87932-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784762.1666918, \"EndTime\": 1712784762.246747, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 79.6515941619873, \"count\": 1, \"min\": 79.6515941619873, \"max\": 79.6515941619873}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:42 INFO 139627334805312] Epoch[163] Batch[0] avg_epoch_loss=1.863519\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=1.8635188341140747\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:45 INFO 139627334805312] Epoch[163] Batch[5] avg_epoch_loss=1.881192\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=1.8811922272046406\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:45 INFO 139627334805312] Epoch[163] Batch [5]#011Speed: 120.07 samples/sec#011loss=1.881192\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:47 INFO 139627334805312] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784762.2468128, \"EndTime\": 1712784767.772095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5525.221109390259, \"count\": 1, \"min\": 5525.221109390259, \"max\": 5525.221109390259}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:47 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.20137162164728 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:47 INFO 139627334805312] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=163, train loss <loss>=1.8744336009025573\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:47 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:48 INFO 139627334805312] Epoch[164] Batch[0] avg_epoch_loss=1.898640\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=1.8986400365829468\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:51 INFO 139627334805312] Epoch[164] Batch[5] avg_epoch_loss=1.875226\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=1.8752259810765584\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:51 INFO 139627334805312] Epoch[164] Batch [5]#011Speed: 120.34 samples/sec#011loss=1.875226\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:53 INFO 139627334805312] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784767.7721677, \"EndTime\": 1712784773.2728555, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5500.246047973633, \"count\": 1, \"min\": 5500.246047973633, \"max\": 5500.246047973633}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:53 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.62927191936677 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:53 INFO 139627334805312] #progress_metric: host=algo-1, completed 82.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=164, train loss <loss>=1.863845670223236\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:53 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:53 INFO 139627334805312] Epoch[165] Batch[0] avg_epoch_loss=1.868421\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=1.8684213161468506\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:56 INFO 139627334805312] Epoch[165] Batch[5] avg_epoch_loss=1.858713\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=1.8587127725283306\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:56 INFO 139627334805312] Epoch[165] Batch [5]#011Speed: 120.45 samples/sec#011loss=1.858713\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:58 INFO 139627334805312] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784773.2729368, \"EndTime\": 1712784778.7627418, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5489.372968673706, \"count\": 1, \"min\": 5489.372968673706, \"max\": 5489.372968673706}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:58 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.48219343052125 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:58 INFO 139627334805312] #progress_metric: host=algo-1, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=165, train loss <loss>=1.8335525751113892\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:58 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:58 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_16bb9fbf-aeb8-483a-896b-3939a4e2c800-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784778.7628126, \"EndTime\": 1712784778.840292, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.11100578308105, \"count\": 1, \"min\": 77.11100578308105, \"max\": 77.11100578308105}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:59 INFO 139627334805312] Epoch[166] Batch[0] avg_epoch_loss=1.849750\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:32:59 INFO 139627334805312] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=1.849750280380249\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:02 INFO 139627334805312] Epoch[166] Batch[5] avg_epoch_loss=1.835278\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=1.8352777361869812\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:02 INFO 139627334805312] Epoch[166] Batch [5]#011Speed: 116.79 samples/sec#011loss=1.835278\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] Epoch[166] Batch[10] avg_epoch_loss=1.835155\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=1.8350082397460938\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] Epoch[166] Batch [10]#011Speed: 106.67 samples/sec#011loss=1.835008\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784778.8403592, \"EndTime\": 1712784785.2847273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6444.311857223511, \"count\": 1, \"min\": 6444.311857223511, \"max\": 6444.311857223511}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=103.96609846433242 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] #progress_metric: host=algo-1, completed 83.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=166, train loss <loss>=1.8351552378047595\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] Epoch[167] Batch[0] avg_epoch_loss=1.807387\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=1.807387113571167\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:33:08 INFO 139627334805312] Epoch[167] Batch[5] avg_epoch_loss=1.896840\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=1.8968397577603657\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:08 INFO 139627334805312] Epoch[167] Batch [5]#011Speed: 119.04 samples/sec#011loss=1.896840\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:11 INFO 139627334805312] Epoch[167] Batch[10] avg_epoch_loss=1.900991\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=1.9059714078903198\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:11 INFO 139627334805312] Epoch[167] Batch [10]#011Speed: 119.31 samples/sec#011loss=1.905971\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:11 INFO 139627334805312] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784785.2847934, \"EndTime\": 1712784791.3638413, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6078.736305236816, \"count\": 1, \"min\": 6078.736305236816, \"max\": 6078.736305236816}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:11 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.72501051419935 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:11 INFO 139627334805312] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=167, train loss <loss>=1.9009905078194358\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:11 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:12 INFO 139627334805312] Epoch[168] Batch[0] avg_epoch_loss=1.843495\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:12 INFO 139627334805312] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=1.8434946537017822\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:14 INFO 139627334805312] Epoch[168] Batch[5] avg_epoch_loss=1.828535\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=1.8285348216692607\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:14 INFO 139627334805312] Epoch[168] Batch [5]#011Speed: 120.25 samples/sec#011loss=1.828535\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:17 INFO 139627334805312] Epoch[168] Batch[10] avg_epoch_loss=1.846505\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=1.8680702447891235\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:17 INFO 139627334805312] Epoch[168] Batch [10]#011Speed: 119.18 samples/sec#011loss=1.868070\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:17 INFO 139627334805312] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784791.3639073, \"EndTime\": 1712784797.4187064, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6054.502964019775, \"count\": 1, \"min\": 6054.502964019775, \"max\": 6054.502964019775}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:17 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=112.64167663785948 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:17 INFO 139627334805312] #progress_metric: host=algo-1, completed 84.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=168, train loss <loss>=1.8465054685419255\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:17 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:18 INFO 139627334805312] Epoch[169] Batch[0] avg_epoch_loss=1.795801\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=1.7958009243011475\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:20 INFO 139627334805312] Epoch[169] Batch[5] avg_epoch_loss=1.866040\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=1.866040309270223\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:20 INFO 139627334805312] Epoch[169] Batch [5]#011Speed: 119.09 samples/sec#011loss=1.866040\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:22 INFO 139627334805312] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784797.4187717, \"EndTime\": 1712784802.959802, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5540.733814239502, \"count\": 1, \"min\": 5540.733814239502, \"max\": 5540.733814239502}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:22 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.09154611615216 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:22 INFO 139627334805312] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:22 INFO 139627334805312] #quality_metric: host=algo-1, epoch=169, train loss <loss>=1.8981552958488463\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:22 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:23 INFO 139627334805312] Epoch[170] Batch[0] avg_epoch_loss=1.862154\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=1.8621537685394287\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:26 INFO 139627334805312] Epoch[170] Batch[5] avg_epoch_loss=1.875168\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=1.8751681447029114\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:26 INFO 139627334805312] Epoch[170] Batch [5]#011Speed: 119.99 samples/sec#011loss=1.875168\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] Epoch[170] Batch[10] avg_epoch_loss=1.902407\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=1.935093092918396\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] Epoch[170] Batch [10]#011Speed: 119.47 samples/sec#011loss=1.935093\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784802.959878, \"EndTime\": 1712784809.0096083, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6049.339056015015, \"count\": 1, \"min\": 6049.339056015015, \"max\": 6049.339056015015}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=107.44804423538548 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] #progress_metric: host=algo-1, completed 85.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=170, train loss <loss>=1.9024067575281316\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] Epoch[171] Batch[0] avg_epoch_loss=1.914993\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:29 INFO 139627334805312] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=1.9149926900863647\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:32 INFO 139627334805312] Epoch[171] Batch[5] avg_epoch_loss=1.916053\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=1.9160527189572651\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:32 INFO 139627334805312] Epoch[171] Batch [5]#011Speed: 119.26 samples/sec#011loss=1.916053\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] Epoch[171] Batch[10] avg_epoch_loss=1.990585\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=2.0800246477127073\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] Epoch[171] Batch [10]#011Speed: 120.03 samples/sec#011loss=2.080025\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784809.009674, \"EndTime\": 1712784815.0707679, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6060.800075531006, \"count\": 1, \"min\": 6060.800075531006, \"max\": 6060.800075531006}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.08994275494709 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=171, train loss <loss>=1.9905854138461025\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] Epoch[172] Batch[0] avg_epoch_loss=1.899486\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=1.8994863033294678\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:33:38 INFO 139627334805312] Epoch[172] Batch[5] avg_epoch_loss=1.900946\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=1.9009457031885784\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:38 INFO 139627334805312] Epoch[172] Batch [5]#011Speed: 118.86 samples/sec#011loss=1.900946\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] Epoch[172] Batch[10] avg_epoch_loss=1.920047\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=1.9429688692092895\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] Epoch[172] Batch [10]#011Speed: 119.43 samples/sec#011loss=1.942969\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784815.0708327, \"EndTime\": 1712784821.1513927, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6080.2671909332275, \"count\": 1, \"min\": 6080.2671909332275, \"max\": 6080.2671909332275}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.02629057974939 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] #progress_metric: host=algo-1, completed 86.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=172, train loss <loss>=1.9200471422889016\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] Epoch[173] Batch[0] avg_epoch_loss=1.821742\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=1.8217415809631348\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:44 INFO 139627334805312] Epoch[173] Batch[5] avg_epoch_loss=1.836446\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=1.8364461064338684\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:44 INFO 139627334805312] Epoch[173] Batch [5]#011Speed: 120.52 samples/sec#011loss=1.836446\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:46 INFO 139627334805312] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784821.1514604, \"EndTime\": 1712784826.6628475, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5511.084794998169, \"count\": 1, \"min\": 5511.084794998169, \"max\": 5511.084794998169}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:46 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.85723311157116 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:46 INFO 139627334805312] #progress_metric: host=algo-1, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=173, train loss <loss>=1.8376733779907226\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:46 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:47 INFO 139627334805312] Epoch[174] Batch[0] avg_epoch_loss=1.793206\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=1.7932063341140747\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:50 INFO 139627334805312] Epoch[174] Batch[5] avg_epoch_loss=1.844322\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=1.8443216284116108\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:50 INFO 139627334805312] Epoch[174] Batch [5]#011Speed: 119.92 samples/sec#011loss=1.844322\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:52 INFO 139627334805312] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784826.66292, \"EndTime\": 1712784832.1812692, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5518.002271652222, \"count\": 1, \"min\": 5518.002271652222, \"max\": 5518.002271652222}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:52 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.25691821744476 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:52 INFO 139627334805312] #progress_metric: host=algo-1, completed 87.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=174, train loss <loss>=1.8426735877990723\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:52 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:52 INFO 139627334805312] Epoch[175] Batch[0] avg_epoch_loss=1.888459\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:52 INFO 139627334805312] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=1.8884587287902832\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:55 INFO 139627334805312] Epoch[175] Batch[5] avg_epoch_loss=1.824777\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:55 INFO 139627334805312] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=1.8247768481572468\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:55 INFO 139627334805312] Epoch[175] Batch [5]#011Speed: 120.55 samples/sec#011loss=1.824777\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] Epoch[175] Batch[10] avg_epoch_loss=1.846828\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=1.8732899665832519\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] Epoch[175] Batch [10]#011Speed: 118.73 samples/sec#011loss=1.873290\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784832.1813412, \"EndTime\": 1712784838.2376733, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6055.930614471436, \"count\": 1, \"min\": 6055.930614471436, \"max\": 6055.930614471436}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.8356193861605 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=175, train loss <loss>=1.8468282656236128\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] Epoch[176] Batch[0] avg_epoch_loss=1.774561\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:33:58 INFO 139627334805312] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=1.7745610475540161\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:01 INFO 139627334805312] Epoch[176] Batch[5] avg_epoch_loss=1.815924\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:01 INFO 139627334805312] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=1.8159240086873372\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:01 INFO 139627334805312] Epoch[176] Batch [5]#011Speed: 119.27 samples/sec#011loss=1.815924\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:04 INFO 139627334805312] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784838.2377427, \"EndTime\": 1712784844.0850713, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5846.945762634277, \"count\": 1, \"min\": 5846.945762634277, \"max\": 5846.945762634277}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:04 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.54960516461428 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:04 INFO 139627334805312] #progress_metric: host=algo-1, completed 88.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=176, train loss <loss>=1.8106112480163574\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:04 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:04 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_5092ced4-b4cf-4839-84c2-79b2cd00a9e4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784844.0851352, \"EndTime\": 1712784844.1622741, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 76.6754150390625, \"count\": 1, \"min\": 76.6754150390625, \"max\": 76.6754150390625}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:04 INFO 139627334805312] Epoch[177] Batch[0] avg_epoch_loss=1.761972\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=1.7619717121124268\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:34:07 INFO 139627334805312] Epoch[177] Batch[5] avg_epoch_loss=1.826177\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=1.826177219549815\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:07 INFO 139627334805312] Epoch[177] Batch [5]#011Speed: 118.98 samples/sec#011loss=1.826177\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] Epoch[177] Batch[10] avg_epoch_loss=1.849424\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=1.877320122718811\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] Epoch[177] Batch [10]#011Speed: 118.88 samples/sec#011loss=1.877320\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784844.162334, \"EndTime\": 1712784850.2445405, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6082.118034362793, \"count\": 1, \"min\": 6082.118034362793, \"max\": 6082.118034362793}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=111.30774837757593 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] #progress_metric: host=algo-1, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=177, train loss <loss>=1.8494239937175403\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] Epoch[178] Batch[0] avg_epoch_loss=1.861715\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=1.8617147207260132\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:13 INFO 139627334805312] Epoch[178] Batch[5] avg_epoch_loss=1.821652\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=1.821651856104533\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:13 INFO 139627334805312] Epoch[178] Batch [5]#011Speed: 118.74 samples/sec#011loss=1.821652\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:16 INFO 139627334805312] Epoch[178] Batch[10] avg_epoch_loss=1.851840\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=1.8880664825439453\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:16 INFO 139627334805312] Epoch[178] Batch [10]#011Speed: 118.85 samples/sec#011loss=1.888066\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:16 INFO 139627334805312] processed a total of 725 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784850.2446074, \"EndTime\": 1712784856.8674278, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6622.519731521606, \"count\": 1, \"min\": 6622.519731521606, \"max\": 6622.519731521606}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:16 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.47321339823476 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:16 INFO 139627334805312] #progress_metric: host=algo-1, completed 89.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:16 INFO 139627334805312] #quality_metric: host=algo-1, epoch=178, train loss <loss>=1.8719745675722759\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:16 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:17 INFO 139627334805312] Epoch[179] Batch[0] avg_epoch_loss=1.908545\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=1.9085447788238525\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:20 INFO 139627334805312] Epoch[179] Batch[5] avg_epoch_loss=1.867148\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=1.8671482006708782\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:20 INFO 139627334805312] Epoch[179] Batch [5]#011Speed: 119.54 samples/sec#011loss=1.867148\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:22 INFO 139627334805312] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784856.8674998, \"EndTime\": 1712784862.4081635, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5540.182113647461, \"count\": 1, \"min\": 5540.182113647461, \"max\": 5540.182113647461}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:22 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.37880729314772 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:22 INFO 139627334805312] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:22 INFO 139627334805312] #quality_metric: host=algo-1, epoch=179, train loss <loss>=1.8533523201942443\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:22 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:23 INFO 139627334805312] Epoch[180] Batch[0] avg_epoch_loss=1.937673\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:23 INFO 139627334805312] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=1.937672734260559\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:25 INFO 139627334805312] Epoch[180] Batch[5] avg_epoch_loss=1.868042\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:25 INFO 139627334805312] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=1.8680415550867717\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:25 INFO 139627334805312] Epoch[180] Batch [5]#011Speed: 119.28 samples/sec#011loss=1.868042\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:27 INFO 139627334805312] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784862.4082358, \"EndTime\": 1712784867.9542816, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5545.621156692505, \"count\": 1, \"min\": 5545.621156692505, \"max\": 5545.621156692505}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:27 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.2234009893573 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:27 INFO 139627334805312] #progress_metric: host=algo-1, completed 90.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=180, train loss <loss>=1.854422676563263\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:27 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:28 INFO 139627334805312] Epoch[181] Batch[0] avg_epoch_loss=1.866634\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:28 INFO 139627334805312] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=1.8666337728500366\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:31 INFO 139627334805312] Epoch[181] Batch[5] avg_epoch_loss=1.838946\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:31 INFO 139627334805312] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=1.8389459053675334\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:31 INFO 139627334805312] Epoch[181] Batch [5]#011Speed: 118.71 samples/sec#011loss=1.838946\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] Epoch[181] Batch[10] avg_epoch_loss=1.890349\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=181, batch=10 train loss <loss>=1.952033233642578\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] Epoch[181] Batch [10]#011Speed: 120.54 samples/sec#011loss=1.952033\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784867.9543765, \"EndTime\": 1712784874.0198324, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6064.97597694397, \"count\": 1, \"min\": 6064.97597694397, \"max\": 6064.97597694397}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.34665096778332 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] #progress_metric: host=algo-1, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=181, train loss <loss>=1.8903492364016445\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] Epoch[182] Batch[0] avg_epoch_loss=1.770738\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:34 INFO 139627334805312] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=1.7707383632659912\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:37 INFO 139627334805312] Epoch[182] Batch[5] avg_epoch_loss=1.837949\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:37 INFO 139627334805312] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=1.8379494945208232\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:37 INFO 139627334805312] Epoch[182] Batch [5]#011Speed: 120.61 samples/sec#011loss=1.837949\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:39 INFO 139627334805312] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784874.0198965, \"EndTime\": 1712784879.511437, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5491.243362426758, \"count\": 1, \"min\": 5491.243362426758, \"max\": 5491.243362426758}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:39 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.81861147431971 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:39 INFO 139627334805312] #progress_metric: host=algo-1, completed 91.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=182, train loss <loss>=1.8245410919189453\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:39 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:40 INFO 139627334805312] Epoch[183] Batch[0] avg_epoch_loss=1.851329\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:40 INFO 139627334805312] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=1.8513288497924805\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:42 INFO 139627334805312] Epoch[183] Batch[5] avg_epoch_loss=1.835538\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:42 INFO 139627334805312] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=1.8355379700660706\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:42 INFO 139627334805312] Epoch[183] Batch [5]#011Speed: 120.18 samples/sec#011loss=1.835538\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:45 INFO 139627334805312] Epoch[183] Batch[10] avg_epoch_loss=1.877810\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=1.9285354614257812\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:45 INFO 139627334805312] Epoch[183] Batch [10]#011Speed: 118.56 samples/sec#011loss=1.928535\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:45 INFO 139627334805312] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784879.5115075, \"EndTime\": 1712784885.5791206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6067.273378372192, \"count\": 1, \"min\": 6067.273378372192, \"max\": 6067.273378372192}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:45 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.11935151889766 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:45 INFO 139627334805312] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:45 INFO 139627334805312] #quality_metric: host=algo-1, epoch=183, train loss <loss>=1.8778095570477573\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:45 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:46 INFO 139627334805312] Epoch[184] Batch[0] avg_epoch_loss=1.855883\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:46 INFO 139627334805312] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=1.8558833599090576\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:34:48 INFO 139627334805312] Epoch[184] Batch[5] avg_epoch_loss=1.865192\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:48 INFO 139627334805312] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=1.86519193649292\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:48 INFO 139627334805312] Epoch[184] Batch [5]#011Speed: 119.05 samples/sec#011loss=1.865192\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:51 INFO 139627334805312] processed a total of 580 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784885.5791867, \"EndTime\": 1712784891.1082957, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5528.816699981689, \"count\": 1, \"min\": 5528.816699981689, \"max\": 5528.816699981689}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:51 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=104.90293434567887 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:51 INFO 139627334805312] #progress_metric: host=algo-1, completed 92.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=184, train loss <loss>=1.821766471862793\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:51 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:51 INFO 139627334805312] Epoch[185] Batch[0] avg_epoch_loss=1.814697\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:51 INFO 139627334805312] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=1.8146965503692627\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:54 INFO 139627334805312] Epoch[185] Batch[5] avg_epoch_loss=1.797935\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:54 INFO 139627334805312] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=1.7979349493980408\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:54 INFO 139627334805312] Epoch[185] Batch [5]#011Speed: 120.32 samples/sec#011loss=1.797935\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] Epoch[185] Batch[10] avg_epoch_loss=1.790227\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=1.7809765338897705\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] Epoch[185] Batch [10]#011Speed: 120.08 samples/sec#011loss=1.780977\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784891.1083672, \"EndTime\": 1712784897.1429822, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6034.2302322387695, \"count\": 1, \"min\": 6034.2302322387695, \"max\": 6034.2302322387695}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.88845157926333 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] #progress_metric: host=algo-1, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=185, train loss <loss>=1.7902265787124634\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_fe7f15d1-5608-4fd8-913d-86f4e85c1fa1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784897.1430483, \"EndTime\": 1712784897.2205267, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 77.11625099182129, \"count\": 1, \"min\": 77.11625099182129, \"max\": 77.11625099182129}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] Epoch[186] Batch[0] avg_epoch_loss=1.788841\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:34:57 INFO 139627334805312] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=1.7888411283493042\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:00 INFO 139627334805312] Epoch[186] Batch[5] avg_epoch_loss=1.820471\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:00 INFO 139627334805312] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=1.8204705913861592\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:00 INFO 139627334805312] Epoch[186] Batch [5]#011Speed: 120.32 samples/sec#011loss=1.820471\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] Epoch[186] Batch[10] avg_epoch_loss=1.785059\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=1.7425644874572754\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] Epoch[186] Batch [10]#011Speed: 112.57 samples/sec#011loss=1.742564\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784897.2205925, \"EndTime\": 1712784903.430827, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6210.179328918457, \"count\": 1, \"min\": 6210.179328918457, \"max\": 6210.179328918457}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=103.53806532585666 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] #progress_metric: host=algo-1, completed 93.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=186, train loss <loss>=1.7850587259639392\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:03 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/state_65fefa3c-dedd-4521-b903-b169173590b8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784903.4308927, \"EndTime\": 1712784903.5129435, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 81.64763450622559, \"count\": 1, \"min\": 81.64763450622559, \"max\": 81.64763450622559}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:04 INFO 139627334805312] Epoch[187] Batch[0] avg_epoch_loss=1.852090\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:04 INFO 139627334805312] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=1.8520902395248413\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:07 INFO 139627334805312] Epoch[187] Batch[5] avg_epoch_loss=1.837284\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:07 INFO 139627334805312] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=1.837283690770467\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:07 INFO 139627334805312] Epoch[187] Batch [5]#011Speed: 119.54 samples/sec#011loss=1.837284\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:09 INFO 139627334805312] Epoch[187] Batch[10] avg_epoch_loss=1.862306\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=1.89233341217041\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:09 INFO 139627334805312] Epoch[187] Batch [10]#011Speed: 119.44 samples/sec#011loss=1.892333\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:09 INFO 139627334805312] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784903.5130208, \"EndTime\": 1712784909.68593, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6172.849655151367, \"count\": 1, \"min\": 6172.849655151367, \"max\": 6172.849655151367}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:09 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.34812661703052 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:09 INFO 139627334805312] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=187, train loss <loss>=1.862306291406805\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:09 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:10 INFO 139627334805312] Epoch[188] Batch[0] avg_epoch_loss=1.876747\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:10 INFO 139627334805312] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=1.8767470121383667\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:13 INFO 139627334805312] Epoch[188] Batch[5] avg_epoch_loss=1.810241\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:13 INFO 139627334805312] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=1.810241123040517\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:13 INFO 139627334805312] Epoch[188] Batch [5]#011Speed: 119.45 samples/sec#011loss=1.810241\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:15 INFO 139627334805312] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784909.6859937, \"EndTime\": 1712784915.2160752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5529.781818389893, \"count\": 1, \"min\": 5529.781818389893, \"max\": 5529.781818389893}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:15 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.2880258162962 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:15 INFO 139627334805312] #progress_metric: host=algo-1, completed 94.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=188, train loss <loss>=1.8125034809112548\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:15 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:15 INFO 139627334805312] Epoch[189] Batch[0] avg_epoch_loss=1.774911\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:15 INFO 139627334805312] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=1.7749109268188477\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:35:18 INFO 139627334805312] Epoch[189] Batch[5] avg_epoch_loss=1.812623\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:18 INFO 139627334805312] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=1.8126231630643208\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:18 INFO 139627334805312] Epoch[189] Batch [5]#011Speed: 120.24 samples/sec#011loss=1.812623\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:20 INFO 139627334805312] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784915.216149, \"EndTime\": 1712784920.7078571, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5491.3599491119385, \"count\": 1, \"min\": 5491.3599491119385, \"max\": 5491.3599491119385}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:20 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=115.26969483467383 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:20 INFO 139627334805312] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:20 INFO 139627334805312] #quality_metric: host=algo-1, epoch=189, train loss <loss>=1.8027162194252013\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:20 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:21 INFO 139627334805312] Epoch[190] Batch[0] avg_epoch_loss=1.818631\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:21 INFO 139627334805312] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=1.8186310529708862\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:24 INFO 139627334805312] Epoch[190] Batch[5] avg_epoch_loss=1.822992\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:24 INFO 139627334805312] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=1.8229916493097942\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:24 INFO 139627334805312] Epoch[190] Batch [5]#011Speed: 118.65 samples/sec#011loss=1.822992\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:26 INFO 139627334805312] Epoch[190] Batch[10] avg_epoch_loss=1.847459\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=1.8768207550048828\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:26 INFO 139627334805312] Epoch[190] Batch [10]#011Speed: 118.10 samples/sec#011loss=1.876821\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:26 INFO 139627334805312] processed a total of 703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784920.707933, \"EndTime\": 1712784926.8326232, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6124.349355697632, \"count\": 1, \"min\": 6124.349355697632, \"max\": 6124.349355697632}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:26 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.78593427637321 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:26 INFO 139627334805312] #progress_metric: host=algo-1, completed 95.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:26 INFO 139627334805312] #quality_metric: host=algo-1, epoch=190, train loss <loss>=1.8474594246257434\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:26 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:27 INFO 139627334805312] Epoch[191] Batch[0] avg_epoch_loss=1.811573\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:27 INFO 139627334805312] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=1.8115729093551636\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:30 INFO 139627334805312] Epoch[191] Batch[5] avg_epoch_loss=1.822054\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:30 INFO 139627334805312] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=1.822054147720337\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:30 INFO 139627334805312] Epoch[191] Batch [5]#011Speed: 119.65 samples/sec#011loss=1.822054\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:32 INFO 139627334805312] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784926.832688, \"EndTime\": 1712784932.3518507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5518.829822540283, \"count\": 1, \"min\": 5518.829822540283, \"max\": 5518.829822540283}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:32 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.0653860070938 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:32 INFO 139627334805312] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:32 INFO 139627334805312] #quality_metric: host=algo-1, epoch=191, train loss <loss>=1.8246709704399109\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:32 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:33 INFO 139627334805312] Epoch[192] Batch[0] avg_epoch_loss=1.869841\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:33 INFO 139627334805312] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=1.8698407411575317\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:35 INFO 139627334805312] Epoch[192] Batch[5] avg_epoch_loss=1.823798\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:35 INFO 139627334805312] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=1.8237978219985962\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:35 INFO 139627334805312] Epoch[192] Batch [5]#011Speed: 120.08 samples/sec#011loss=1.823798\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:38 INFO 139627334805312] Epoch[192] Batch[10] avg_epoch_loss=1.834688\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=1.8477564334869385\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:38 INFO 139627334805312] Epoch[192] Batch [10]#011Speed: 118.54 samples/sec#011loss=1.847756\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:38 INFO 139627334805312] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784932.3519194, \"EndTime\": 1712784938.4212239, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6068.88747215271, \"count\": 1, \"min\": 6068.88747215271, \"max\": 6068.88747215271}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:38 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.36324599470426 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:38 INFO 139627334805312] #progress_metric: host=algo-1, completed 96.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:38 INFO 139627334805312] #quality_metric: host=algo-1, epoch=192, train loss <loss>=1.8346880999478428\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:38 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:39 INFO 139627334805312] Epoch[193] Batch[0] avg_epoch_loss=1.805401\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:39 INFO 139627334805312] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=1.8054007291793823\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:41 INFO 139627334805312] Epoch[193] Batch[5] avg_epoch_loss=1.831858\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:41 INFO 139627334805312] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=1.8318581183751423\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:41 INFO 139627334805312] Epoch[193] Batch [5]#011Speed: 120.00 samples/sec#011loss=1.831858\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:43 INFO 139627334805312] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784938.4212923, \"EndTime\": 1712784943.9296596, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5508.0671310424805, \"count\": 1, \"min\": 5508.0671310424805, \"max\": 5508.0671310424805}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:43 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=114.55708086380679 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:43 INFO 139627334805312] #progress_metric: host=algo-1, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:43 INFO 139627334805312] #quality_metric: host=algo-1, epoch=193, train loss <loss>=1.8139767050743103\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:43 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:44 INFO 139627334805312] Epoch[194] Batch[0] avg_epoch_loss=1.754254\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:44 INFO 139627334805312] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=1.7542544603347778\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:47 INFO 139627334805312] Epoch[194] Batch[5] avg_epoch_loss=1.778781\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:47 INFO 139627334805312] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=1.7787814736366272\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:47 INFO 139627334805312] Epoch[194] Batch [5]#011Speed: 120.71 samples/sec#011loss=1.778781\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:49 INFO 139627334805312] Epoch[194] Batch[10] avg_epoch_loss=1.827311\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=1.8855458498001099\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:49 INFO 139627334805312] Epoch[194] Batch [10]#011Speed: 119.79 samples/sec#011loss=1.885546\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:49 INFO 139627334805312] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784943.9297311, \"EndTime\": 1712784949.9581542, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6027.931690216064, \"count\": 1, \"min\": 6027.931690216064, \"max\": 6027.931690216064}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:49 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=113.96761904716703 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:49 INFO 139627334805312] #progress_metric: host=algo-1, completed 97.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:49 INFO 139627334805312] #quality_metric: host=algo-1, epoch=194, train loss <loss>=1.8273107355291194\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:49 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:50 INFO 139627334805312] Epoch[195] Batch[0] avg_epoch_loss=1.795381\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:50 INFO 139627334805312] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=1.7953811883926392\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:53 INFO 139627334805312] Epoch[195] Batch[5] avg_epoch_loss=1.827590\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:53 INFO 139627334805312] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=1.827589988708496\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:53 INFO 139627334805312] Epoch[195] Batch [5]#011Speed: 120.31 samples/sec#011loss=1.827590\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] Epoch[195] Batch[10] avg_epoch_loss=1.821680\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=1.8145889520645142\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] Epoch[195] Batch [10]#011Speed: 118.48 samples/sec#011loss=1.814589\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784949.9582193, \"EndTime\": 1712784956.030182, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6071.63143157959, \"count\": 1, \"min\": 6071.63143157959, \"max\": 6071.63143157959}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=108.53582389815898 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=195, train loss <loss>=1.8216804265975952\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] Epoch[196] Batch[0] avg_epoch_loss=1.808034\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:56 INFO 139627334805312] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=1.8080344200134277\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/10/2024 21:35:59 INFO 139627334805312] Epoch[196] Batch[5] avg_epoch_loss=1.835864\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:59 INFO 139627334805312] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=1.8358641465504963\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:35:59 INFO 139627334805312] Epoch[196] Batch [5]#011Speed: 119.18 samples/sec#011loss=1.835864\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:02 INFO 139627334805312] Epoch[196] Batch[10] avg_epoch_loss=1.829019\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=1.8208054065704347\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:02 INFO 139627334805312] Epoch[196] Batch [10]#011Speed: 113.34 samples/sec#011loss=1.820805\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:02 INFO 139627334805312] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784956.0302477, \"EndTime\": 1712784962.2461958, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6215.620517730713, \"count\": 1, \"min\": 6215.620517730713, \"max\": 6215.620517730713}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:02 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.88273200226094 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:02 INFO 139627334805312] #progress_metric: host=algo-1, completed 98.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:02 INFO 139627334805312] #quality_metric: host=algo-1, epoch=196, train loss <loss>=1.8290192647413774\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:02 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:03 INFO 139627334805312] Epoch[197] Batch[0] avg_epoch_loss=1.778862\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:03 INFO 139627334805312] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=1.7788622379302979\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:05 INFO 139627334805312] Epoch[197] Batch[5] avg_epoch_loss=1.798379\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:05 INFO 139627334805312] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=1.798378547032674\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:05 INFO 139627334805312] Epoch[197] Batch [5]#011Speed: 118.34 samples/sec#011loss=1.798379\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:08 INFO 139627334805312] Epoch[197] Batch[10] avg_epoch_loss=1.818029\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=197, batch=10 train loss <loss>=1.8416102886199952\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:08 INFO 139627334805312] Epoch[197] Batch [10]#011Speed: 119.62 samples/sec#011loss=1.841610\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:08 INFO 139627334805312] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784962.246262, \"EndTime\": 1712784968.5118165, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 6265.223979949951, \"count\": 1, \"min\": 6265.223979949951, \"max\": 6265.223979949951}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:08 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=106.61867962704783 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:08 INFO 139627334805312] #progress_metric: host=algo-1, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:08 INFO 139627334805312] #quality_metric: host=algo-1, epoch=197, train loss <loss>=1.8180293386632747\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:08 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:09 INFO 139627334805312] Epoch[198] Batch[0] avg_epoch_loss=1.858022\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:09 INFO 139627334805312] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=1.85802161693573\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:11 INFO 139627334805312] Epoch[198] Batch[5] avg_epoch_loss=1.864419\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:11 INFO 139627334805312] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=1.8644189437230427\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:11 INFO 139627334805312] Epoch[198] Batch [5]#011Speed: 119.69 samples/sec#011loss=1.864419\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:14 INFO 139627334805312] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784968.5118804, \"EndTime\": 1712784974.0381866, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5526.007175445557, \"count\": 1, \"min\": 5526.007175445557, \"max\": 5526.007175445557}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:14 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=109.48026389985826 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:14 INFO 139627334805312] #progress_metric: host=algo-1, completed 99.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=198, train loss <loss>=1.8451248288154602\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:14 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:14 INFO 139627334805312] Epoch[199] Batch[0] avg_epoch_loss=1.852091\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:14 INFO 139627334805312] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=1.8520909547805786\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:17 INFO 139627334805312] Epoch[199] Batch[5] avg_epoch_loss=1.793939\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:17 INFO 139627334805312] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=1.7939387162526448\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:17 INFO 139627334805312] Epoch[199] Batch [5]#011Speed: 120.02 samples/sec#011loss=1.793939\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784974.038257, \"EndTime\": 1712784979.5893924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5550.761461257935, \"count\": 1, \"min\": 5550.761461257935, \"max\": 5550.761461257935}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] #throughput_metric: host=algo-1, train throughput=110.7935538048192 records/second\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] #quality_metric: host=algo-1, epoch=199, train loss <loss>=1.8359493732452392\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] Final loss: 1.7850587259639392 (occurred at epoch 186)\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] #quality_metric: host=algo-1, train final_loss <loss>=1.7850587259639392\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 WARNING 139627334805312] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 WARNING 139627334805312] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:19 INFO 139627334805312] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784979.589463, \"EndTime\": 1712784980.4152856, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 825.0956535339355, \"count\": 1, \"min\": 825.0956535339355, \"max\": 825.0956535339355}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:20 INFO 139627334805312] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784980.4153554, \"EndTime\": 1712784980.7282746, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 1138.1278038024902, \"count\": 1, \"min\": 1138.1278038024902, \"max\": 1138.1278038024902}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:20 INFO 139627334805312] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:20 INFO 139627334805312] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784980.7283413, \"EndTime\": 1712784980.7825453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 54.16083335876465, \"count\": 1, \"min\": 54.16083335876465, \"max\": 54.16083335876465}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:20 INFO 139627334805312] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:20 INFO 139627334805312] #memory_usage::<batchbuffer> = 7.63671875 mb\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:20 INFO 139627334805312] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784980.7826002, \"EndTime\": 1712784980.7890909, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.02956390380859375, \"count\": 1, \"min\": 0.02956390380859375, \"max\": 0.02956390380859375}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784980.7891393, \"EndTime\": 1712784982.371572, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 1582.5083255767822, \"count\": 1, \"min\": 1582.5083255767822, \"max\": 1582.5083255767822}}}\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, RMSE): 39.467405059004115\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, mean_absolute_QuantileLoss): 3898.229013167487\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, mean_wQuantileLoss): 0.4991997631780099\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.1]): 0.21150307723868583\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.2]): 0.3023276381013963\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.3]): 0.3793030876815089\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.4]): 0.4503696933857559\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.5]): 0.5171759625886074\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.6]): 0.5798126988831871\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.7]): 0.6376517301567173\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.8]): 0.6890677900950318\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #test_score (algo-1, wQuantileLoss[0.9]): 0.7255861904711993\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #quality_metric: host=algo-1, test RMSE <loss>=39.467405059004115\u001b[0m\n",
      "\u001b[34m[04/10/2024 21:36:22 INFO 139627334805312] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.4991997631780099\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1712784982.3716545, \"EndTime\": 1712784982.46378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 4.476785659790039, \"count\": 1, \"min\": 4.476785659790039, \"max\": 4.476785659790039}, \"totaltime\": {\"sum\": 1183191.2910938263, \"count\": 1, \"min\": 1183191.2910938263, \"max\": 1183191.2910938263}}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-04-10 21:36:42 Uploading - Uploading generated training model\n",
      "2024-04-10 21:36:42 Completed - Training job completed\n",
      "Training seconds: 1411\n",
      "Billable seconds: 1411\n",
      "CPU times: user 3.35 s, sys: 199 ms, total: 3.55 s\n",
      "Wall time: 24min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a816d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "612d1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "551d3254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: deepar-gas-prices-2024-04-10-21-37-15-218\n",
      "INFO:sagemaker:Creating endpoint-config with name deepar-gas-prices-2024-04-10-21-37-15-218\n",
      "INFO:sagemaker:Creating endpoint with name deepar-gas-prices-2024-04-10-21-37-15-218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a0f13d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0.1        0.5        0.9\n",
      "2021-04-29  60.372284  63.622448  66.555328\n",
      "2021-05-06  51.439186  54.181522  57.336132\n",
      "2021-05-13  54.002087  57.436928  60.840626\n",
      "2021-05-20  47.974461  50.865154  54.422993\n",
      "2021-05-27  47.035351  49.235153  51.521553\n",
      "...               ...        ...        ...\n",
      "2024-02-29  17.615688  19.561876  20.889048\n",
      "2024-03-07  17.139095  19.088638  20.905512\n",
      "2024-03-14  17.316238  20.402014  22.787571\n",
      "2024-03-21  18.120401  20.737875  22.911348\n",
      "2024-03-28  18.353308  20.641817  24.169909\n",
      "\n",
      "[153 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "predictions = predictor.predict(timeseries[0][start_dataset : end_training - timedelta(days=1)])\n",
    "predictions.to_csv('DeepArResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "43bb3cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.figure(figsize = (9,6))\n",
    "pyplot.plot(predictions.index, predictions[\"0.5\"], color='green', label='DeepAR')\n",
    "pyplot.plot(predictions.index, test_data[0]['target'][len(training_data[0]['target']):], color='blue', label='Actual')\n",
    "pyplot.xlabel(\"Date\")\n",
    "pyplot.ylabel(\"Gas Price\")\n",
    "pyplot.title(\"Deep AR\")\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61e76b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: deepar-gas-prices-2024-04-02-22-10-36-088\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: deepar-gas-prices-2024-04-02-22-10-36-088\n",
      "INFO:sagemaker:Deleting endpoint with name: deepar-gas-prices-2024-04-02-22-10-36-088\n"
     ]
    }
   ],
   "source": [
    "# REMEMBER TO RUN THIS\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
